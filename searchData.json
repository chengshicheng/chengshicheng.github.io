[{"title":"多进程 - multiprocessing","url":"/2020/04/11/multi-process/","content":"\nPython提供了非常好用的多进程包multiprocessing，只需要定义一个函数，Python会完成其他所有事情。借助这个包，可以轻松完成从单进程到**并发执行**的转换。\n\n##  性能测试\n\n上一篇提到了对于CPU密集型程序，多进程理论上可以显著提高性能，那还是继续用上一篇的示例代码，用`Process` 实现多进程来测试一下性能：\n\n```python\nimport time\nfrom multiprocessing import Process\n\n\ndef add(n):\n    sum = 0\n    while sum < n:\n        sum += 1\n    print(f'sum:{sum}')\n\n\nif __name__ == '__main__':\n    n = 500000000\n    #  单线程\n    start = time.time()\n    add(n)\n    print('run time1: %s  s' % str(time.time() - start))\n\n    # 多进程\n    start = time.time()\n    p1 = Process(target=add, args=[n // 2])\n    p2 = Process(target=add, args=[n // 2])\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n    print('run time2: %s s' % str(time.time() - start))\n\n```\n\n运行结果：\n\n```shell\nsum:500000000\nrun time1: 19.68217897415161  s\nsum:250000000\nsum:250000000\nrun time2: 11.469464540481567 s\n```\n\n很明显看到，不同于上篇中的多线程，多进程明显提升了纯计算程序的性能；多进程也常用于web 应用部署，搭配协程的使用，能大幅度提升web应用的并发性能，这个后面再单独介绍。下面着重介绍一下Process类。\n\n## Process  \n\n####  使用方法\n\nProcess 类可用来在多操作系统平台上创建新进程。和使用 Thread 类创建多线程方法类似，使用 Process 类创建多进程也有以下 2 种方式：\n\n1. 直接创建 Process 类的实例对象，由此就可以创建一个新的进程；\n2. 通过继承 Process 类的子类，创建实例对象，也可以创建新的进程。注意，继承 Process 类的子类需重写父类的 run() 方法。\n\n不仅如此，Process 类中也提供了一些常用的属性和方法，如表 1 所示。\n\n| **属性名或方法名** | **功能**                                                     |\n| :----------------- | ------------------------------------------------------------ |\n| run()              | 第 2 种创建进程的方式需要用到，继承类中需要对方法进行重写，该方法中包含的是新进程要执行的代码。 |\n| start()            | 和启动子线程一样，新创建的进程也需要手动启动，该方法的功能就是启动新创建的线程。 |\n| join([timeout])    | 和 thread 类 join() 方法的用法类似，其功能是在多进程执行过程，其他进程必须等到调用 join() 方法的进程执行完毕（或者执行规定的 timeout 时间）后，才能继续执行； |\n| is_alive()         | 判断当前进程是否还活着。                                     |\n| terminate()        | 中断该进程。                                                 |\n| name属性           | 可以为该进程重命名，也可以获得该进程的名称。                 |\n| **daemon**         | 和守护线程类似，通过设置该属性为 True，可将新建进程设置为“守护进程”。 |\n| pid                | 返回进程的 ID 号。大多数操作系统都会为每个进程配备唯一的 ID 号。 |\n\n和使用 thread 类创建子线程的方式非常类似，使用 方式1中的方法来创建Process 类对象，其本质是调用该类的构造方法创建新进程。Process 类的构造方法格式如下：\n\ndef __init__(self,group=None,target=None,name=None,args=(),kwargs={})\n\n其中，各个参数的含义为：\n\n- group：该参数未进行实现，不需要传参；\n- target：为新建进程指定执行任务，也就是指定一个函数；\n- name：为新建进程设置名称；\n- args：为 target 参数指定的参数传递非关键字参数；\n- kwargs：为 target 参数指定的参数传递关键字参数。\n\n####  守护进程\n\ndaemon 为守护进程标志，其实在多线程中也有daemon守护线程的概念，属性同理，设置daemon属性必须在调用start() 方法之前，否则会报RuntimeError 错误。\n\n- daemon=True。运行时如果如果主进程结束，那么子进程也随之结束运行，如果在守护进程中子进程加了join(timeout)（起到阻塞主进程的作用），那么主进程会等子进程都运行完。\n\n  timeout为等待超时时间，可以不指定：\n\n  - 不指定时，在子进程执行结束后，会自动唤醒主线程。\n  - 指定时，如果守护进程执行时间为t1，会在min(t1,timeout)，主动唤醒主线程\n\n- daemon=False。False为默认值，主进程执行完自己的任务以后，就退出了，此时子进程会继续执行自己的任务。join(timeout)参数作用同上。\n\njoin方法的原理就是调用相应线程/进程的wait方法进行等待操作的，例如主线程中调用了A线程/进程的join方法，则相当于在主线程/进程中调用了B线程/进程的wait方法，当B线程/进程执行完（或者到达等待时间），B会自动调用自身的notifyAll方法唤醒主线程/进程继续执行，从而达到同步的目的。\n\n提到守护进程，就不得不提僵尸进程和孤儿进程的概念了\n\n#### 僵尸进程\n\n**僵尸进程：(父进程没结束，子进程提前结束，父进程没有处理子进程的状态)——-有害，应当避免**\n一个进程使用fork创建子进程，如果子进程退出，而父进程没有调用wait或waitpid获取进程的状态信息，那么子进程的进程描述符仍保存在系统中，这种进程称为僵死进程。\n\n#### 孤儿进程\n\n**孤儿进程：(父进程提前退出，子进程还没结束，子进程成为孤儿进程)——–无害**\n一个父进程退出，而它的一个或着多个子进程还在运行，那么这些子进程将称为孤儿进程。孤儿进程将被init进程（进程号1）所收养, 并由init进程对它们完成状态收集工作。\n\n*参考博客：https://blog.csdn.net/Lovegengxin/article/details/80347468*\n\n##  Pool  \n\n#### 使用方法\n\nmultiprocessing 模块提供了 Pool类，专门用来创建一个进程池。\n\n> multiprocessing.Pool( processes )\n\n进程池可以提供指定数量的进程给用户使用，即当有新的请求提交到进程池中时，如果池未满，则会创建一个新的进程用来执行该请求；反之，如果池中的进程数已经达到规定最大值，那么该请求就会等待，只要池中有进程空闲下来，该请求就能得到执行。其中，processes 参数用于指定该进程池中包含的进程数。如果进程是 None，则默认使用 os.cpu_count() 返回的数字。\n\n#### 属性方法\n\n| 方法名                                                       | 功能                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| apply( func[, args[, kwds]] )                                | 将 func 函数提交给进程池处理。其中 args 代表传给 func 的位置参数，kwds 代表传给 func 的关键字参数。该方法会被阻塞直到 func 函数执行完成。 |\n| apply_async( func[, args[, kwds[, callback[, error_callback]]]] ) | 这是 apply() 方法的异步版本，该方法不会被阻塞。其中 callback 指定 func 函数完成后的回调函数，error_callback 指定 func 函数出错后的回调函数。 |\n| map( func, iterable[, chunksize] )                           | 类似于 Python 的 map() 全局函数，只不过此处使用新进程对 iterable 的每一个元素执行 func 函数。 |\n| map_async( func, iterable[, chunksize[, callback[, error_callback]]] ) | 这是 map() 方法的异步版本，该方法不会被阻塞。其中 callback 指定 func 函数完成后的回调函数，error_callback 指定 func 函数出错后的回调函数。 |\n| imap( func, iterable[, chunksize] )                          | 这是 map() 方法的延迟版本。                                  |\n| imap_unordered( func, iterable[, chunksize] )                | 功能类似于 imap() 方法，但该方法不能保证所生成的结果（包含多个元素）与原 iterable 中的元素顺序一致。 |\n| starmap( func, iterable[,chunksize] )                        | 功能类似于 map() 方法，但该方法要求 iterable 的元素也是 iterable 对象，程序会将每一个元素解包之后作为 func 函数的参数。 |\n| close()                                                      | 关闭进程池。在调用该方法之后，该进程池不能再接收新任务，它会把当前进程池中的所有任务执行完成后再关闭自己。 |\n| terminate()                                                  | 立即中止进程池。                                             |\n| join()                                                       | 等待所有进程完成。                                           |\n\n演示demo：\n\n```python\nfrom multiprocessing import Pool\n\ndef add(n):\n    sum = 0\n    while sum < n:\n        sum += 1\n    print(f'sum:{sum}')\n\nif __name__ == '__main__':\n    #创建包含 4 条进程的进程池\n    with Pool(processes=4) as pool:\n        adds = pool.map(add, (200,400,600,800))\n```\n\n## 结语\n\n多线程和多进程相关的文章只写了大致的应用场景，稍微总结了一点 api 的使用，不过这都不是重点，真正的重点是多线程/进程之间互斥锁和共享数据、以及多线程/进程通信的多重通信方式原理，后面有空再慢慢更。。。"},{"title":"Python 多线程探究","url":"/2020/01/17/multi-thread/","content":"\n# Python 多线程探究\n\n### GIL锁\n\n在介绍Python的多线程编程之前，需要想明确一个概念，Python中的多线程是一个假的多线程，在Python的原始解析器CPython中存在着全局解释器锁（Global Interpreter Lock，GIL），GIL在任何时候都确保只有一个Python线程执行，这个过程中会产生互斥锁来限制线程对共享资源的访问，直到解释器遇到I/O操作或者操作次数达到一定数目时才会释放GIL。\n\n### 多线程性能测试\n\n那么多线程性能表现如何？性能测试一下：\n\n```python\nimport time, threading\n\n\ndef add(n):\n    sum = 0\n    while sum < n:\n        sum += 1\n    print(f'sum:{sum}')\n\n\nif __name__ == '__main__':\n    n = 500000000\n\n    #  单线程\n    start = time.time()\n    add(n)\n    print('run time1: %s  s' % str(time.time() - start))\n\n    # 多线程\n    start = time.time()\n    t1 = threading.Thread(target=add, args=[n // 2])\n    t2 = threading.Thread(target=add, args=[n // 2])\n    t1.start()\n    t2.start()\n    t1.join()\n    t2.join()\n    print('run time2: %s s' % str(time.time() - start))\n\n```\n\n执行结果\n\n```bash\nsum:500000000\nrun time1: 37.292006969451904 s\nsum:250000000\nsum:250000000\nrun time2: 37.5808539390564 s\n```\n\n结果表明此处的多线程并没有什么性能提升，反而由于GIL的频繁设置和释放，性能可能还不如单线程。解释的通俗点，以上场景就像在食堂打饭，单线程的时候是一个阿姨负责一个窗口，多线程是一个阿姨负责两个窗口，可是多开的一个窗口，并没有提升工作效率。\n\nGIL是理解了，那么问题来了，多线程意义何在？还是上面那个例子，假设每个同学在打饭的时候，需要到窗口点菜，打菜的过程其实很快，但是有些同学这时候纠结症就犯了，看菜单花个半分钟，点菜纠结半分钟，阿姨表示很无奈，先去别的窗口招呼同学去了。这么一看，好像就比一个窗口要快很多了，感觉阿姨一个人招呼四五个窗口不成问题(´･ᆺ･`)\n\n接下来用延时（ 或者网络请求等耗时操作）来模拟同学们的拖延症，在add()函数中加一行代码`time.sleep(1)`:\n\n```python\n    while sum < n:\n         time.sleep(1)\n         sum += 1\n```\n\n为了避免用时过长，把add()方法入参 n 改成20，运行结果如下：\n\n```python\nsum:20\nrun time1: 20.021007776260376 s\nsum:10\nsum:10\nrun time2: 10.010624170303345 s\n```\n\n运行结果令人欣慰，运行效率提升max，那么仅一行代码，多线程带来的性能提升为何如此之大？这里涉及到一对概念：CPU密集型、IO密集型。\n\n\n\n### 什么是CPU密集型、I/O 密集型？\n\n来自 [stackoverflow  What do the terms “CPU bound” and “I/O bound” mean?](https://stackoverflow.com/questions/868568/what-do-the-terms-cpu-bound-and-i-o-bound-mean)\n\n- CPU密集型\n\n  A program is CPU bound if it would go faster if the CPU were faster, i.e. it spends the majority of its time simply using the CPU (doing calculations). A program that computes new digits of π will typically be CPU-bound, it's just crunching numbers.\n\n  如果CPU变得更快时，程序运行得更快，则该程序是CPU密集型 ，程序的大部分时间都用于CPU（进行计算）。计算π的数值程序就是个典型的CPU密集型，它只用来计算数字。\n\n  \n\n- I / O密集型\n\n  A program is I/O bound if it would go faster if the I/O subsystem was faster. Which exact I/O system is meant can vary; I typically associate it with disk, but of course networking or communication in general is common too. A program that looks through a huge file for some data might become I/O bound, since the bottleneck is then the reading of the data from disk (actually, this example is perhaps kind of old-fashioned these days with hundreds of MB/s coming in from SSDs).\n\n  如果I / O子系统更快，则程序运行得更快，则该程序是I / O密集型。确切的I / O系统的含义可能会有所不同。我通常将其与磁盘联系在一起，但是一般来说，在网络通信中也很常见。在大文件中查找数据的程序很可能是I/O 密集型，因为瓶颈是从磁盘读取数据（实际上，这个例子可能已经过时，因为SSD已经达到了数百MB/S的速度了）\n\n  \n\n### 应用场景\n\n大部分任务都可以分为CPU密集型和I / O密集型。\n\nCPU密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。\n\nCPU密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。\n\n第二种任务的类型是I / O密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于I / O密集型任务，任务越多，CPU处于等待的时间就越短，CPU的效率相对会提高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。\n\nIO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率高的语言，脚本语言是首选。\n\n\n\n总之，Python就不应该用来写CPU密集型的程序，毕竟运行效率实在太低。。。\n\n\n\n### 结语\n\n大多数情况下，性能瓶颈往往不是单一的CPU或者I / O引起的，当两者都存在时，任意一方的速度提升，都能影响到程序最终的执行效率。\n\n在Python语言中有没有更好的解决方案呢？当然是有的，那就是并行编程（parallel programmning），下一篇继续介绍。\n\n","tags":["python"]},{"title":"Flask 开发之Model：Flask-SQLAlchemy","url":"/2020/01/08/flask-sqlalchemy/","content":"### 前言\n\n由于日常工作中经常在`Flask`和`Django`切换，不同的ORM框架属性区别还是有点大的，在`Flask`中使用了常见的`SQLAlchemy`ORM框架，而`SQLAlchemy`的官方文档又相当简洁，仅仅提供了基本的增删改查操作实例。语法倒是不复杂，只是长时间不接触的话，一时半会还真想不起来，因此在这里总结记录一下。\n\n本文使用[`Flask-SQLAlchemy`](https://pypi.org/project/Flask-SQLAlchemy/)，和`SQLAlchemy`相比语法上稍有不同，具体不再赘述。\n\n### 基础操作\n\n##### 查询\n```python\n# 根据主键查询\nUser.query.get(2)\nUser.query.get_or_404(2)\nUser.query.filter(User.phone=='1316418xxxx').all()\nUser.query.filter(User.phone=='1316418xxxx').first()\nUser.query.filter(User.phone=='1316418xxxx').one()\nUser.query.filter(User.phone=='1316418xxxx').one_or_none()\n\n# first()返回的是tuple格式\n>>> User.query.with_entities(User.username,User.phone).filter(User.phone=='1316418xxxx').first()\n('chengshicheng', '1316418xxxx')\n\n# scalar()返回唯一个结果的第一个元素\n>>> User.query.with_entities(User.username,User.phone).filter(User.phone=='1316418xxxx').scalar()\n'chengshicheng'\n```\n需要注意的是`one()`和`scalar()`方法会抛出`MultipleResultsFound`和`NoResultFound`的异常，`one_or_none()`方法会抛出`MultipleResultsFound`异常。使用的时候需要注意捕捉相关异常。\n\n##### 修改\n```python\n# 修改model\nu = User.query.get(2)\nu.phone = '1327777xxxx'\ndb.session.commit()\n\n# update\nUser.query.filter(User.phone == '1316418xxxx').update({'phone':'1327777xxxx'})\ndb.session.comm\n```\n\n##### 删除\n```python\n# 删除model\nu = User.query.get(2)\ndb.session.delete(u)\ndb.session.commit()\n\n# 使用delete()\nUser.query.filter(User.phone=='1316418xxxx').delete(synchronize_session=False)\ndb.session.commit()\n```\n批量数据删除时，使用query.delete()可以绕过ORM体系，能够显著提升性能。\n\nsynchronize_session  选择从会话中操作匹配对象的策略：\n\n- False \n\n    不同步session，直接执行删除操作。，如果被删除的 objects 已经在 session 中存在，在 session commit 或者 expire_all 之前，这些被删除的对象都存在 session 中。可能会导致获取被删除 objects 时出错。不同步可能会导致获取被删除 objects 时出错\n    \n- 'fetch'    \n\n    删除之前从 db 中匹配被删除的对象并保存在 session 中，然后再从 session 中删除\n    \n- 'evaluate'   \n\n    默认值。根据当前的 query criteria 扫描 session 中的 objects，如果不能正确执行则抛出错误\n\n### 复杂查询\n\n##### filter过滤\n```python\n# equal\nUser.query.filter(User.phone == '1316418xxxx').first()\n\n# not equal\nUser.query.filter(User.phone != '1316418xxxx').first()\nUser.query.filter(not_(User.phone == '1316418xxxx')).first()\n\n# in/not in\nUser.query.filter(User.id.in_([1,2,3])).all()\nUser.query.filter(User.id.notin_([1,2,3])).all()\n\n# None 判断\nUser.query.filter(User.phone.is_(None)).all()\nUser.query.filter(User.phone.isnot(None)).all()\n\n# like/ilike\nUser.query.filter(User.name.like('%CHENG%')).all()\nUser.query.filter(User.name.ilike('%cheng%')).all() #ilike(不区分大小写)\nUser.query.filter(User.phone.notlike('%CHENG%')).all()\nUser.query.filter(User.phone.notilike('%cheng%')).all()\n\n# 文本匹配（都可以用like实现）\nUser.query.filter(User.phone.startwith('1316418')).all()\nUser.query.filter(User.phone.endwith('xxxx')).all()\nUser.query.filter(User.phone.contains('6418')).all()\n\n# and_/or_/not_ 分别对应 & | ~ 运算符\nUser.query.filter(User.age < 18).filter(User.register_at < '2019-01-01').all()\nUser.query.filter(User.age < 18, User.register_at < '2019-01-01').all()\nUser.query.filter(and_(User.age < 18, User.register_at < '2019-01-01')).all()\nUser.query.filter((User.age < 18) & (User.register_at < '2019-01-01')).all()\n\nUser.query.filter(or_(User.age < 18, User.age > 60)\nUser.query.filter((User.age < 18) | (User.age > 60))\n\nUser.query.filter(not_(User.age < 18)).all()\nUser.query.filter(~(User.age < 18)).all()\n\n#组合查询\nOrder.query.filter(Order.created_at > '2019-01-01', order.shop_id= 1,\n                not_(Order.status='unpaid'),\n                or_(Order.type==3, Order.type==7)).all()\n```\n\n##### limit offset slice\n```python\nUser.query.offset(5).limit(10).all()\nUser.query.slice(0,2).all()\nUser.query[0:2]\nUser.query[0]\n```\n\n##### order_by 排序\n```python\nfrom sqlalchemy.sql import text\n\nUser.query.order_by(User.id.desc()).all()\nUser.query.order_by(text('-id')).all()\n\n# id逆序，参考slice操作\nUser.query[::-1]\n\nUser.query.order_by(User.id.asc()).all()\nUser.query.order_by(text('id')).all()\n```\n\n##### 聚合\n```python\nfunc.count:统计行的数量\nfunc.avg:求平均值\nfunc.max:求最大值\nfunc.min:求最小值\nfunc.sum:求和\n\n# 用户的平均年龄和最大年龄\nfrom sqlalchemy import func\nUser.query.with_entities(func.avg(User.age),func.max(User.age)).first()\n```\n\n##### group_by\n```python\nfrom sqlalchemy import func\n\n#group_by 按年龄分组统计人数\nUser.query.with_entities(User.age,func.count()).group_by(User.age).all()\n\n#having 分组后结果再过滤\nUser.query.with_entities(User.age,func.count()).group_by(User.age).having(User.age>18).all()\n\n# group_concat拼接\nUser.query.with_entities(User.age,func.group_concat(User.id),func.count()).group_by(User.age).all()\n\n```\n##### join 联表查询\n```python\n\n# 默认外键关联\nOrder.query.filter(Order.status='paid',Order.created_at > '2019-01-01')\n    .join(User, user.phone == '1316418xxxx')\n# 指定外键关联\nOrder.query.filter(Order.status='paid',Order.created_at > '2019-01-01')\n    .join(User, and_(User.id == Order.user_id, user.phone == '1316418xxxx'))\n\n```\n##### subquery 子查询\n\n```python\n>>> subq  = User.query.with_entities(User.id).filter(User.phone=='1316418xxxx').subquery()\n>>> Order.query.filter(Order.created_by == subq.c.id).all()\n>>> print(Order.query.filter(Order.created_by == a.c.id))\nSELECT orders.id AS orders_id, ...\nFROM orders, \n(SELECT users.id AS id FROM users WHERE users.phone = %s) AS anon_1 \nWHERE orders.created_by = anon_1.id\n\n\n>>> u_ids = User.query.with_entities(User.id).filter(User.phone=='1316418xxxx')\n>>> print(Order.query.filter(Order.created_by.in_(u_ids.subquery())))\nSELECT orders.id AS orders_id, ...\nFROM orders \nWHERE orders.created_by IN (SELECT users.id \nFROM users \nWHERE users.phone = %s)\n\n```\n\n##### label 别名\n\n```python\nUser.query.with_entities((User.username + '_' + User.phone).label('userinfo')).all()\nUser.query.with_entities(User.id.label('user_id')).order_by(text('-user_id')).all()\n\nuser_id = User.id.label('user_id')\nUser.query.filter(user_id>4).order_by(user_id.desc()).all()\n\n# 按月统计订单数量(过去一年内的)\nq = Order.query.with_entities(func.date_format(Order.created_date,'%Y-%m').label('month'),func.count())\n\t.group_by('month').order_by('month').limit(12)\n\n>>> print(q)\nSELECT date_format(orders.created_date, %s) AS month, count(*) AS count_1 \nFROM orders GROUP BY month ORDER BY month \n LIMIT %s\n```\n\n目前用的比较多的就是这些，应该满足大部分情况下的查询，后续会持续更新。如有特别复杂的情况，也可以直接执行sql语句。\n\n##### union 、unionall 区别\n\n```python\nfollowed = Post.query.join(\n            followers, (followers.c.followed_id == Post.user_id)).filter(\n                followers.c.follower_id == self.id)\n        own = Post.query.filter_by(user_id=self.id)\n        return followed.union(own)\n```\n\nsql缓存\n\n```\nq=User.query.prefix_with('SQL_CACHE')\nprint(q)\nSELECT SQL_CACHE users.id AS users_id, users.phone AS users_phone, users.username AS users_username, users.password_hash AS users_password_hash, users.is_active AS users_is_active, users.joined AS users_joined, users.last_login AS users_last_login, users.role_id AS users_role_id \nFROM users\n```\n\n","tags":["Flask","SQLAlchemy"]},{"title":"Selenium 爬虫总结","url":"/2019/12/15/spider-summary/","content":"## 1 前言\n最近由于项目需要，需要抓取网络上的一下数据，实现之后写个技术总结吧。其实我个人是不建议将爬虫接入到正式的项目中的，一来是有爬虫就有反爬虫，程序能稳定多久完全靠运气，网站的接口数据或页面格式稍有变动，爬虫就挂了，这种还是比较好解决的；一旦反扒机制升级，那就是噩梦的开始。二来其实会对服务器造成压力，如果爬取数据量大的话，一般网站也许就扛不住了，一旦造成服务器瘫痪，约等于网络攻击，最后一点。也是最重要的，爬虫可能是违法行为，这个取决于你爬取的数据类型和具体用途，也取决于对方想不想追究你的责任。总之一句话，且爬且珍惜。。。\n\n开始爬虫之前，有以下几个建议：\n\n1. 能通过接口拿到的，不要通过浏览器拿。爬虫的目的是要得到数据，能通过API直接拿到，不仅速度快，还不用解析界面。\n2. 不要只盯着PC端看，多看看移动端的网页或者API，会有惊喜。\n\n在确定要爬谁之前，先开个Fiddler,Charles等顺手的抓包工具，PC端，APP端，MobileWeb端，甚至小程序端，看一下能不到找带到现成的API。实在不行页面+API混着用也可以。接口需要认证权限的话还需要再寻找如何绕过去。\n\n一般来说，PC网页端的数据是最难爬的，反爬机制也是做的最好的，移动端的相对来说会弱很多。\n## 2 环境搭建\n\n在PhantomJS停止更新之后，selenium已经不建议使用PhantomJS了，建议使用chrome和firefox浏览器。如果是在自己电脑上运行的话，只要有浏览器就可以，如果要在服务器上运行的话，\n由于服务器没有安装桌面环境，所以浏览器只能运行在headless模式下，记录一下在Ubuntu Server安装浏览器的步骤：\n\n```shell\npip install selenium\n```\n在调用selenium启动浏览器的时候，需要指定浏览器的可执行文件路径，可以用webdriver_manager来替我们管理浏览器。\n```shell\npip install webdriver_manager\n```\n#### 2.1 chrome\n```shell\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo dpkg -i google-chrome*.deb\n```\n如果环境依赖报错，执行以下命令：\n```shell\nsudo dpkg -i  --force-depends google-chrome*.deb  \nsudo apt-get install -f\n```\n\n#### 2.2 firefox\n```shell\nsudo apt-get install firefox\n```\n\n\n## 3 IP代理\n\n#### 3.1 设置代理\n- requests\n    ```python\n  proxies = {'http': f\"http://{host}:{port}\",\n               'https': f\"http://{host}:{port}\", }\n  s = requests.Session()\n  s.proxies.update(proxies)\n  url = 'http://www.baidu.com'\n  resp = s.get(url)\n  ```\n- selenium+chrome\n    ```python\n    from selenium import webdriver\n    \n    options =  webdriver.chrome.options.Options()\n    options.add_argument(f'--proxy-server=http://{host}:{port}')\n    driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n    driver.get('http://www.baidu.com')\n    ```\n\n- selenium+firefox\n    ```python\n  from selenium import webdriver\n  from webdriver_manager.firefox import GeckoDriverManager\n  \n  fp = webdriver.FirefoxProfile()\n  # Direct = 0, Manual = 1, PAC = 2, AUTODETECT = 4, SYSTEM = 5\n  fp.set_preference(\"network.proxy.type\", 1)\n  fp.set_preference(\"network.proxy.http\", host)\n  fp.set_preference(\"network.proxy.http_port\", int(port))\n  fp.set_preference(\"network.proxy.ssl\", host)\n  fp.set_preference(\"network.proxy.ssl_port\", int(port))\n  fp.update_preferences()\n\n  driver = webdriver.Firefox(GeckoDriverManager().install(),\n                           firefox_profile=fp, options=options)\n  driver.get('http://www.baidu.com')\n  ```\n\n\n\n#### 3.2 匿名ip检验\n在设置代理后，可以访问一下页面，查看ip代理是否生效：http://myip.ipip.net/\n```python\n当前 IP：223.71.7.223  来自于：中国 广东 深圳  电信\n```\n\n到这一步还不够，真正的高匿名ip是不会暴露客户端的ip的。\n终极大杀器：http://httpbin.org/get?show_env=1\n\nHeader内容一览无余，不仅仅可以检验IP代理，还可以查看Header内容伪造是否成功\n```python\n{\n  \"args\": {\n    \"show_env\": \"1\"\n  }, \n  \"headers\": {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\", \n    \"Accept-Language\": \"en,zh;q=0.9,zh-CN;q=0.8\", \n    \"Dccept-Encoding\": \"none\", \n    \"Dnt\": \"1\", \n    \"Host\": \"httpbin.org\", \n    \"Upgrade-Insecure-Requests\": \"1\", \n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\", \n    \"X-Forwarded-For\": \"223.71.7.223, 223.71.7.223\", \n    \"X-Forwarded-Port\": \"80\", \n    \"X-Forwarded-Proto\": \"https\", \n    \"X-Real-Ip\": \"223.71.7.223\"\n  }, \n  \"origin\": \"223.71.7.223, 223.71.7.223\", \n  \"url\": \"https://httpbin.org/get?show_env=1\"\n}\n```\n\n非高匿名IP代理请求，在Header中是会包暴露客户端IP的(X-Forwarded-For)，在Header中能同时看到客户端和代理的IP，我的代理是直接付费购买的，还没有遇到过这种情况。\n\n## 4 webdriver设置\n\n在PhantomJS停止更新之后，seleniums已经不建议使用PhantomJS了，不过新版chrome和firefox都支持了headless模式，下面的参数是我用到的，更多参数可以参考官方文档：\n\n#### 4.1 headlss模式\n- selenium+chrome\n    ```python\n  options = webdriver.chrome.options.Options()\n  options.add_argument('--headless')\n   driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n\n  ```\n- selenium+firefox\n    ```python\n  options = webdriver.firefox.options.Options()\n  options.add_argument('--headless')\n  driver = webdriver.Firefox(options=options)\n  ```\n  或者\n  ```python\n  options = webdriver.firefox.options.Options()\n  options.headless = True  \n  driver = webdriver.Firefox(options=options)\n  ```\n  \n#### 4.2 不加载图片\n省去图片能够提高页面的加载速度\n- selenium+chrome\n```python\n# 1允许所有图片；2阻止所有图片；3阻止第三方服务器图片\noptions.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2,})\n\n```\n- selenium+firefox\n```python\noptions.set_preference('permissions.default.image', 2)\n```\n\n#### 4.3 设置user-agent\n我感觉修改这个用处不大，毕竟热门的useragent可能就那么多，就算反爬策略监测到了某个user-agent频繁请求，总不能给封了吧，封了之后其他正常用户还让不让用了。\n安装fake_useragent: `pip install fake_useragent`，或者自己存一点随机取。\n- selenium+chrome\n```python\nfrom fake_useragent import UserAgent\noptions.add_argument(f'user-agent={UserAgent().chrome}')\n```\n- selenium+firefox\n```python\nfp = webdriver.FirefoxProfile()\nfp.set_preference(\"general.useragent.override\", UserAgent().firefox)\ndriver = webdriver.Firefox(firefox_profile=fp)\n```\n\n#### 4.4 避免爬虫监测\n很多资料显示，selenium控制下的浏览器会被监测到，页面通过js代码获取到的`window.navigator.webdriver` 值不一样，正常手动打开的浏览器获取到的是`undefined`，而用selenium开启的浏览器是`true`。在浏览器中按下F12，点击console\n- 手动打开chrome\n```\n> window.navigator.webdriver\n< undefined\n```\n- selenium打开chrome\n```\n> window.navigator.webdriver\n< true\n```\n\n看来这个问题却是是存在的，至于网站有没有做这个监测，可以看一下js代码。很多资料都是用mitmproxy屏蔽js代码来绕过监测，实现起来也比较麻烦，不过对于chrome浏览器来说，设置成开发者模式就可以很轻松的解决，只需要一行代码\n```python\noptions.add_experimental_option('excludeSwitches', ['enable-automation'])\n```\n\n#### 4.5 其他设置\n- 等待时间，超时时间\n```python\ndriver = webdriver.xxxx()\ndriver.implicitly_wait(20)\ndriver.set_page_load_timeout(20)\n```\nimplicitly_wait是页面等待时长，经过实际验证，如果实际页面x秒(x<20)加载完成，则代码执行结束，否则页面继续等待加载至总时长（20秒）,才回执行后面的流程，\nset_page_load_timeout是超时时间，如果不设置，在find_element方法招不到对应元素的时候程序会一直卡住，所以这个参数一定要设置。\n\n\n## 5 元素解析\n推荐chrome插件xpath helper，鼠标往元素上一放就可以得到xpath，再也不用分析html层级了。本地开发调试时可以先关闭headless模式，可以直观看到页面数据\n\n## 6 其他\n以上示例中，每次新建一个webdriver都会新建一个浏览器窗口，所以代码每次处理完毕之后记得关闭浏览器，否则开多了系统内存也扛不住了。\n\n## 7 demo\n```python\nimport json\nimport time\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException, TimeoutException\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom fake_useragent import UserAgent\n\nhost = 'xxx.xx.xxx.xx'\nport = 'xx'\n\noptions = webdriver.chrome.options.Options()\noptions.add_argument('--headless')\noptions.add_argument('--no-sandbox')\noptions.add_argument('--disable-gpu')\n# 1允许所有图片；2阻止所有图片；3阻止第三方服务器图片\noptions.add_experimental_option(\"prefs\", {\n    \"profile.managed_default_content_settings.images\": 2,\n})\noptions.add_argument(f'user-agent={UserAgent().chrome}')\n# 开发者模式\noptions.add_experimental_option('excludeSwitches', ['enable-automation'])\n\n# 添加代理服务器信息\noptions.add_argument(f'--proxy-server=http://{host}:{port}')\ndriver = webdriver.Chrome(ChromeDriverManager().install(),\n                          chrome_options=options)\ndriver.implicitly_wait(20)\ndriver.set_page_load_timeout(20)  # 一定要设置超时时间,否则element找不到会一直卡死\ndata = {}\ntry:\n    driver.get('http://httpbin.org/get?show_env=1')\n    pre = driver.find_element_by_xpath(\"/html/body/pre\").text\nexcept NoSuchElementException as e1:\n    print(f'error,browser page:{driver.title}')\nexcept TimeoutException as e2:\n    print(f'webdriver time out')\nelse:\n    data = json.loads(pre)\nfinally:\n    #driver.delete_all_cookies()  # 清除浏览器cookie缓存\n    driver.quit()\n\nprint(data)\n```\n\n","tags":["python","爬虫"]},{"title":"Django自定义中间件","url":"/2019/11/16/django-mid/","content":"## 前言\nDjango中间件是一个常规的Python类，可插入Django的请求/响应生命周期。所有Django请求都会执行注册过的中间件中的方法，自定义的中间件需要继承Middleware类，并在项目设置中注册路径MIDDLEWARE_CLASSES。\n\n自定义中间件类应至少定义以下方法之一：\n\n-  在请求期间调用：\n\t- process_request(request)\n\t- process_view(request, view_func, view_args, view_kwargs)\n\t- process_exception（request，exception)\n\n-  在响应期间调用：\n\t- process_exception(request, exception) \n\t- process_template_response(request, response)\n\t- process_response(request, response)\n\n看方法名称就能看出来都是干什么的了，根据自己需要新建自定义类即可。\n\n## 中间件原理\n在整个请求到相应的过程中，会两次调用Middleware类，所以在设置的时候需要注意中间件的顺序，先看一下我当前项目设置的Middleware类：\n```python\nMIDDLEWARE_CLASSES = [\n    'corsheaders.middleware.CorsMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'django.middleware.common.CommonMiddleware',\n]\n```\n在请求周期中，会**自上而下**的执行Middleware类，即首先执行CorsMiddleware类，依次执行到CommonMiddleware类，对于每个中间件，它都会执行process_request()和process_view()方法。\n接下来Django将会在view中执行，完成view应该完成的工作（查询数据库，分页，处理信息，逻辑处理等等），然后为客户端返回`Response`响应。\n\n在响应周期中，将**自下而上**执行Middleware类，即首先执行CommonMiddleware，最后执行 CorsMiddleware。同理，对于每个中间件，它将执行process_exception()，process_template_response()和process_response()方法。\n\n以上都执完毕之后，才回返回响应到客户端。下图是从Django官方文档中提取的，图解很形象。\n[![](https://docs.djangoproject.com/en/1.8/_images/middleware.svg)](https://docs.djangoproject.com/en/1.8/_images/middleware.svg)\n\n\n## 创建中间件类\n在我目前项目中，对于一些捕捉到的异常，是有用log记录异常信息的，但是没有捕捉到的exception，在非development环境中，是看不到的，所以打算加一个全局捕捉异常的中间件，用来trace 异常信息，方便快速定位问题。\n\n首先在lib文件夹下新建一个exception. py文件：\n```\nimport logging\nfrom django.utils.deprecation import MiddlewareMixin\nlogger = logging.getLogger(__name__)\n\nclass LogExceptionMiddleware(MiddlewareMixin):\n\n    def process_exception(self, request, exception):\n        import traceback\n        logger.error(traceback.format_exc())\n\n```\n\n接下来去`setting.py` 文件中添加`LogExceptionMiddleware`到Django中间件类集合中即可：\n```python\nMIDDLEWARE_CLASSES = [\n    ...\n    'lib.exeption.LogExceptionMiddleware'\n]\n\n```\n\n启动服务，随便抛个异常，就能在log文件中看到异常信息了。\n\n","tags":["Django","python"]},{"title":"python网络通信：ctypes和struct使用总结","url":"/2019/09/21/ctypes-struct/","content":"\n最近项目组需要对接一个用C语言开发的so库，so库的函数主要功能是封装了一些socket通信的细节，以及数据加密。so的相关文档介绍了使用方式，主要流程如下：\n1. 初始化环境\n2. 创建socket\n3. 注册函数，socket的返回数据会通过这个函数返回\n4. 通过socket发送字节数据\n5. 接受返回数据，并解析处理\n\n第一次看的时候还是有点懵逼的，首先，动态库是C程序，这个函数是Python程序，C程序能够直接调用Python程序吗？其次，虽然这里不管是C还是Python都涉及到一些基本的数据类型，比如int型变量，C中的int和Python中的int是一样的吗？很显然，这两个问题的答案都是否定的，C程序没法直接调用Python函数，C中的整型和Python中的整型虽然都标记为int，但是两者是不一样的。\n\n这就是[`ctypes`](https://docs.python.org/3.6/library/ctypes.html)库存在的原因了，我们都知道Python是用C语言开发的，具体内部实现细节我们不得而知，但是像上面提到的，int,string等基本的数据类型，如何和C对应起来，要知道C中的类型可比Python复杂多了。ctypes库作为C和Python之间的桥梁，让Python程序也可以和C语言交互了。\n\n## ctypes使用\n\n### 数据类型对应\nctypes定义了许多基本的C兼容数据类型：\n\nctypes type     | C type                                  | Python type\n---             | ---                                     | ---\nc_bool          | _Bool                                   | bool (1)\nc_char          | char                                    | 1-character bytes object\nc_wchar\t        | wchar_t\t                              | 1-character string\nc_byte\t        | char\t                                  | int\nc_ubyte\t        | unsigned char\t                          | int\nc_short\t        | short\t                                  | int\nc_ushort\t    | unsigned short\t                      | int\nc_int\t        | int\t                                  | int\nc_uint\t        | unsigned int\t                          | int\nc_long\t        | long\t                                  | int\nc_ulong\t        | unsigned long\t                          | int\nc_longlong\t    | __int64 or long long\t                  | int\nc_ulonglong\t    | unsigned __int64 or unsigned long long  | int\nc_size_t\t    | size_t\t                              | int\nc_ssize_t\t    | ssize_t or Py_ssize_t\t                  | int\nc_float\t        | float\t                                  | float\nc_double\t    | double\t                              | float\nc_longdouble\t| long double\t                          | float\nc_char_p\t    | char * (NUL terminated)                 | bytes object or None\nc_wchar_p\t    | wchar_t * (NUL terminated)              | string or None\nc_void_p\t    | void *\t                              | int or None\n\n\n\n### 加载so  \nPython通过ctypes加载so有两种方式，如果so文件不在系统环境变量路径中，也可以使用相对路径或者绝对路径来加载，`libc.so.6` 库是linux系统中自带的c函数库，常用的c函数都能通过它调用。\n```python\n>>> from ctypes import *\n>>> libc = CDLL(\"libc.so.6\") \n>>> libc\n<CDLL 'libc.so.6', handle 7fb8246af000 at 0x7fb823860940>\n>>> \n>>> libc=cdll.LoadLibrary(\"/usr/lib/libc.so.6\")\n>>> libc\n<CDLL '/usr/lib/libc.so.6', handle 7f6d0ff77000 at 0x7f6d0f129710>\n```\n\n### 函数调用\n```python\n>>> func=libc.printf\n>>> func\n<_FuncPtr object at 0x7f2320a594f8>\n>>> result = func(c_char_p('hello'))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: bytes or integer address expected instead of str instance\n>>> \n>>> result = func(c_char_p('hello'.encode()))\nhello>>> \n>>> result\n5\n```\nC的printf()函数接收字节数组的指针`char *`，对应ctypes中的`c_char_p`，直接传入字符串会报错，提示传入`bytes`，转化后成功输出。\n\n总而言之，C类型接收什么类型的参数，Python调用时用ctypes内置的方法转换一下就可以了。其实有一些不用转换也可以直接调用，比如int类型：\n\n```python\n>>> libc.abs(-100)\n100\n```\n\n\n### 指定参数类型\n\n在调用C函数时，可以通过前置的指定类型方法，为我们检查数据格式是否正确，比如有这样一个函数：\n```c\nint CSend_message(char *data,int len)\n```\n入参分别是需要发送的参数，和参数长度，返回值是int类型，代表是否发送成功。在Python中可以用`argtypes`和`restype`来指定其入参和返回值的类型，ctypes会为我们检查输入的参数类型是否正确：\n\n```python\n>>> data = 'to_send_data'.encode()\n>>> \n>>> so = CDLL(\"xxx.so\") \n>>> csend = so.CSend_message\n>>> csend.argtypes = [c_char_p, c_int]\n>>> csend.restype = c_int\n>>> result = csend(data, len(data))\n\n```\n\n当然简单的参数我觉得是用不上这些的，在比较复杂的情况下，还是比较试用的，比如这个：\n```c\nint CSign(unsigned char sourcedata[],unsigned long sourcedataLen,\n          unsigned char targetdata[],unsigned long &targetdataLen);\n          \n参数说明：\n\n1. sourcedata 未签名的数据    \n2. sourcedataLen 未签名数据长度    \n3. targetdata 已签名数据 \n4. targetdataLen 已签名数据长度的地址 \n```\n\n将要加密的参数Sourcedata传入后，该方法会将加密后的128字节的数据存入targetdata所在的内存地址中，要想拿到加密结果，需要我们自己从该地址中读取，这里随意初始化一个128字节的字节数组传入即可：\n\n```python\n>>> data = 'to_sign_data'.encode()\n>>> length = len(data)\n>>> source = tuple(z for z in data)\n>>> source_data  = (c_ubyte * length)(*source)\n>>> source_data\n<__main__.c_ubyte_Array_12 object at 0x7f2320ab29d8>\n>>> \n>>> target_data = (c_ubyte * 128)(1)\n>>> target_data\n<__main__.c_ubyte_Array_128 object at 0x7f2320a7eae8>\n>>> \n>>> sign = so.CSign\n>>> sign.argtypes = [c_ubyte * length, c_ulong,\n                     c_ubyte * 128, c_ulong]\n>>> sign.restype = c_int\n>>> sign(source_data, c_ulong(length),\n         target_data, c_ulong(addressof(c_int(128)))\n         )\n```\n\n### 地址取值\n上面的加密程序执行之后，需要从`target_data` 所在的内存地址中取值。ctypes中的取值我用到了两种，\n一种是根据内存地址来取值，一种是根据指针来取值，其实可以总结为一种，因为指针指向的就是一个内存地址。\n\n- 通过`addressof`方法可以获取ctypes对象的内存地址，再通过`string_at()`取出固定大小的数据\n   从内存地址取值：\n    ```python\n    string_at(addressof(target_data), 128)\n    ```\n- 通过`cast`方法将ctypes对象转化为指针对象，再用`string_at()`取出数据\n    ```python\n    data_p = cast(target_data, c_char_p)\n    string_at(data_p, 128)\n    ```\n\n### 回调函数\n\n#### 创建回调函数\n上面提到socket发送数据之后，server端的数据会通过回调函数返回，文档中对回调函数的定义如下，返回两个参数，第一个是指向数据的指针，第二个是数据的长度：\n\n```c\nint (CALLBACK *ReadCallback)(char *data, int len)\n```\n\n根据要求在Python中新建回调函数：\n```python\ndef callback(data_p, len):\n    message = string_at(data_p, len)\n    parse_message(message)\n    return 0\n```\n\n函数也很简单，就是从指针地址中取出数据，并调用数据解析函数，返回值为int。\n\n#### 注册回调函数\n\n在创建socket客户端的时候，输入服务器的ip和端口，同时传入回调函数，C函数定义如下\n```c\nint CCreate_socket(ReadCallback readcallback,char *ip,u_short port);\n```\n\n在ctypes中，通过关键字 `CFUNCTYPE` 来声明一个回调函数的类型，通过关键字的字面意思也能够看出来，C-FUNC-TYPE，那就是C语言的函数（FUNC）类型（TYPE）。\n```python\nCMPFUNC = CFUNCTYPE(c_int, c_char_p, c_int)\n```\n眼尖的同学可能已经发现了，上面不是求只要获得回传参数的指针和长度么，这里声明回调函数类型的时候怎么变成了三个参数？其实吧，这里只是指的数据类型，并不是参数本身，后面两个 `c_char_p`和`c_int` 指的是传递给 `callback` 回调函数的参数类型，第一个 `c_int` 指的是回调函数的返回值类型。只有这样定义，C程序才能认识。\n\npython相关的代码如下：\n```python\nsocket = ukey.CCreate_socket\nsocket.argtypes = [CMPFUNC, c_char_p, c_ushort]\nsocket.restype = c_int\nc = socket(CMPFUNC(callback), ip, port)\n```\n\n\n## struct打包拆包\n\nstruct是python用来组包解包常用的包，在网络通信中，所有的数据都会通过字节流的方式传输，比如常用的 `https` ，只不过这些通信协议帮我们实现了底层的数据拼接和发送的细节，让开发者只需要关注应用层的业务逻辑。在`socket`通信中，一般都是开发者自定义协议规则。\n\n在ctypes的数据类型对应表中，可以看到C中的数据类型很多，Python中的一个`int`的数据，在组包发送的时候，需要对应C中的哪一种数据类型，到底应该填充几个字节呢？在协议定好的基础上，通过struct可以轻松定义。使用比较简单，只需要掌握`字节序` 和 `字符格式` 两点。\n\n\n#### 字节序\n对于多字节数据，由于在内存中的存储方式的不同，可以分为`大端` 和 `小端`模式；默认情况下，C类型以机器的本机格式和字节顺序表示， struct可以指定具体用哪种模式：\n\n\nCharacter  | Byte order                  | Size\n---        | ---                         | ---\n@          | native                      | native\n=          | native                      | standard  \n<          | little-endian               | standard \n>          | big-endian                  | standard\n!          | network (= big-endian)      | standard\n\n\n#### 字符格式\n\n格式字符具有以下含义；C 和 Python值之间的按其指定类型的转换应当是直接定义具体类型，每种类型的标准大小，就是打包后占用的字节个数：\n\n\n格式        | C类型                | Python类型 | 标准大小 | 注释\n---        | ---                  | ---       | ---    | ---\nx          | 填充字节               | 无         | 1     |  (1),(3)\nc          | char       | 长度为 1 的字节串 | 1   |  (3)\nb          | signed char        | 整数 | 1   |  (1)\nB          | unsigned char      | 整数 | 1   |  (3)\n?          | _Bool      | bool | 2   |  (3)\nh          | short      | 整数 | 2   |  (3)\nH          | unsigned short     | 整数 | 4   |  (3)\ni          | int        | 整数 | 4   |  (3)\nI          | unsigned int       | 整数 | 4   |  (3)\nl          | long       | 整数 | 4   |  (2), (3)\nL          | unsigned long      | 整数 | 8   |  (2), (3)\nq          | long long      | 整数 | 8   |  (4)\nQ          | unsigned long long     | 整数 |    |  (4)\nn          | ssize_t        | 整数 |    |  (5)\nN          | size_t     | 整数 | 2   |  (5)\ne          | (7)        | 浮点数 | 4   |  (5)\nf          | float      | 浮点数 | 8   |  \nd          | double     | 浮点数 |    |  \ns          | char[]     | 字节串 |    |  (6)\np          | char[]     | 字节串 |    |  \nP          | void *     | 整数 |    |  \n\n\n#### pack/unpack示例\n`pack`进行打包，`unpack`进行解包，`calcsize` 用于计算字符格式的大小。常用的就是这3个方法。\n\n\n\n小端模式打包解包`short`、 `unsigned int`和`unsigned long`三个数据:\n```python\n>>> import struct\n>>> struct.pack('<hIL',1,2,3)\nb'\\x01\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n>>> struct.unpack('<hIL',b'\\x01\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00')\n(1, 2, 3)\n>>> struct.calcsize('hIL')\nstr(    struct  \n>>> struct.calcsize('<hIL')\n10\n>>> \n```\n\n大小端的区别：\n```python\n>>> struct.pack('<L', 1)\nb'\\x01\\x00\\x00\\x00'\n>>> struct.pack('>L', 1)\nb'\\x00\\x00\\x00\\x01'\n>>> \n```\n\n格式字符的顺序可能对大小产生影响，因为满足对齐要求所需的填充是不同的:\n```python\n>>> struct.pack('ci', b'*', 7654321)\nb'*\\x00\\x00\\x00\\xb1\\xcbt\\x00'\n>>> struct.pack('ic', 7654321, b'*')\nb'\\xb1\\xcbt\\x00*'\n>>> struct.calcsize('ci')\n8\n>>> struct.calcsize('ic')\n5\n```\n\n格式字符的标准大小会对大小产生影响，区别于是数据的标准大小还是原生大小：\n```python\n>>> struct.calcsize('<hIL')\n10\n>>> struct.calcsize('@hIL')\n16\n```\n\n多个连续的可以在前面加数字，`hhh` 等于 `3h`：\n```python\n>>> struct.pack('hhh', 1,2,3)\nb'\\x01\\x00\\x02\\x00\\x03\\x00'\n>>> struct.pack('3h', 1,2,3)\nb'\\x01\\x00\\x02\\x00\\x03\\x00'\n>>> \n```\n\nstruct还有`pack_into` 和`unpack_from` 用于从固定offset来处理数据，用的不多在这里就不在介绍了。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["python"]},{"title":"Linux gcc编译动态、静态链接库","url":"/2019/09/07/gcc-so/","content":"\n\n# 库的分类\n\n### 动态链接库\n动态库的链接是在程序执行的时候被链接的。所以，即使程序编译完，依赖库仍须保留在系统上，以供程序运行时调用。如果依赖库文件找不到了，动态链接库就无法正常运行了。\n\n###  静态链接库\n静态链接库不受依赖库的影响，即使依赖库被删除了，程序依然可以运行成功。\n\n\n两种库文件应该是各有利弊，静态链接库的本质其实是赋值粘贴的过程，编译过程中会将所使用的库文件一起打包嵌入到可执行文件中。相比之下，链接动态库的程序体积会小很多。\n\n接下类将分别编译静态和动态库，最后动手看看具体有什么区别。首先新建`static` 和`dynamic`文件夹，用于存放各自代码。\n\n\n# 编译动态库\n进入 `dynamic` 文件夹，新建 `add.c` 文件：\n\n```c\n#include <stdio.h>\n\nint add(int a,int b)\n{\n\tint c;\n\tc = a+b;\n   \tprintf(\"from c add() : %d\\n\",c);\n   \treturn c;\n}\n```\n运行以下命令编译，会生成 `libadd.so` 文件：\n\n```\ngcc -fPIC -shared add.c -o libadd.so\n```\n实际上上述过程分为编译和链接两步：\n\n-fPIC是编译选项，PIC是 Position Independent Code的缩写，表示要生成位置无关的代码，这是动态库需要的特性； \n\n-shared是链接选项，告诉gcc生成动态库而不是可执行文件。\n\n上述的一行命令等同于下面两个命令，第一行是把 `add.c` 编译成可执行文件 `add.o`, 如果在C程序中加入 `main()` 函数，是可以直接运行 `add.o` 文件的；第二行是通过 `-shared` 参数将 `add.o` 实现动态链接，得到 `libadd.so` 。\n```\ngcc -c -fPIC add.c\ngcc -shared -o libadd.so add.o\n```\n\n接下来我们将新建 `test.c` 来测试刚刚生成的动态链接库\n首先创建 `add.h` 头文件\n\n```c\nvoid add();\n```\n\n写入测试程序到 `test.c` ：\n\n```\n#include <stdio.h>\n#include \"add.h\"\n \nint main(){\n\tprintf(\"call add in test.c\\n\");\n\tadd(2,3);\n}\n```\n\n编译 `test.c` 文件：\n\n```\ngcc test.c -L. -ladd -o test\n```\n-L的选项是指定编译器在搜索动态库时搜索的路径，告诉编译器 `add.so`库的位置。\".\"意思即当前路径，如果不指定，编译器会找不到依赖库而报错\n\n```\n➜  dynamic gcc test.c -ladd -o test\n/usr/bin/ld: 找不到 -ladd\ncollect2: 错误：ld 返回 1\n```\n\n-l选项是因为\nLinux下的库文件在命名时有一个约定，就是应该以 lib 这3个字母开头，由于所有的库文件都遵循了同样的规范，因此在用 -l 选项指定链接的库文件名时可以省去 lib 这3个字母。例如，gcc 在对 `-ladd ` 进行处理时，会自动去`-L` 指定的文件夹下链接名为 `libadd.so` 的文件。\n\n编译完成后，文件夹下有以下文件：\n\n```zsh\n➜  dynamic ls\nadd.c  add.h  libadd.so  test  test.c\n```\n\n我们刚刚在 `test.c`文件中是定义了 `main()` 函数的，此时编译后的 `test` 应该是可以直接运行的。\n\n```\n➜  dynamic ./test\n./test: error while loading shared libraries: libadd.so: cannot open shared object file: No such file or directory\n➜  dynamic ldd test.so\n\tlinux-vdso.so.1 (0x00007ffc859df000)\n\tlibadd.so => not found\n\tlibc.so.6 => /usr/lib/libc.so.6 (0x00007f72e33fb000)\n\t/lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007f72e35fb000)\n➜  dynamic\n```\n\n报错信息提示找不到` libadd.so` 文件，通过 `ldd` 命令查看依赖库时也提示未找到，最简单的方式是将依赖库文件拷贝到系统目录：\n\n```\n➜  dynamic  sudo cp libadd.so /usr/lib\n[sudo] chengshicheng 的密码：\n➜  dynamic \n➜  dynamic ./test\ncall add in test.c\nfrom c add() : 5 \n➜  dynamic \n```\n\n这样，再次执行就成功了，可以看到在`main()` 函数中调用了`libadd.so`动态库中的方法，并计算成功。\n\n\n# 编译静态库\n\n静态库代码和动态库基本一样,只是gcc编译命令参数和动态链接库的有一些差别。先进入 `static` 文件夹，新建 `add.c` 文件：\n\n```c\n#include <stdio.h>\n\nint add(int a,int b)\n{\n\tint c;\n\tc = a+b;\n   \tprintf(\"from c add() : %d\\n\",c);\n   \treturn c;\n}\n```\n\n编译 `add.c` 文件，会生成 `add.o`文件，注意到这里和编译动态链接库时的命令就是少了`-fPIC`参数：\n\n```\ngcc -c add.c\n```\n生成静态库\n\n```\nar -r libadd.a add.o\n```\n\n现在可以开始测试刚刚生成的静态链接库 `libmyadd.a`，首先新建 `add.h`头文件：\n\n\n```\nvoid add();\n```\n然后新建测试文件 `test.c` ：\n\n```\n#include <stdio.h>\n#include \"add.h\"\n \nint main(){\n\tprintf(\"call add in test.c\\n\");\n\tadd(3,5);\n}\n```\n编译运行\n```zsh\n➜  static gcc test.c -ladd -L. -static -o test\n➜  static ls\nadd.c  add.h  add.o  libadd.a  test  test.c\n➜  static ./test\ncall add in test.c\nfrom c add() : 8\n➜  static \n````\n\n要验证依赖库确实是静态库，我们移除静态库   `libadd.a` 文件试试看：\n\n```zsh\n➜  static mv libadd.a libadd.a.back\n➜  static ls\nadd.c  add.h  add.o  libadd.a.back  test  test.c\n➜  static ./test \ncall add in test.c\nfrom c add() : 8\n➜  static \n\n```\n\n以上，静态链接库经过验证之后没有问题。上文中我们提到过链接静态库会比动态的文件体积大，那我们来对比下，直接上数据：\n\n```\n➜  static ll\n总用量 764K\n-rw-r--r-- 1 chengshicheng chengshicheng  114  9月  6 00:59 add.c\n-rw-r--r-- 1 chengshicheng chengshicheng   13  9月  6 01:10 add.h\n-rw-r--r-- 1 chengshicheng chengshicheng 1.6K  9月  6 01:21 add.o\n-rw-r--r-- 1 chengshicheng chengshicheng 1.7K  9月  7 00:14 libadd.a.back\n-rwxr-xr-x 1 chengshicheng chengshicheng 743K  9月  7 00:21 test\n-rw-r--r-- 1 chengshicheng chengshicheng   96  9月  6 01:09 test.c\n➜  static ll ../dynamic \n总用量 48K\n-rw-r--r-- 1 chengshicheng chengshicheng 114  9月  6 00:46 add.c\n-rw-r--r-- 1 chengshicheng chengshicheng  12  9月  5 23:56 add.h\n-rwxr-xr-x 1 chengshicheng chengshicheng 16K  9月  6 00:46 libadd.so\n-rwxr-xr-x 1 chengshicheng chengshicheng 17K  9月  7 00:18 test\n-rw-r--r-- 1 chengshicheng chengshicheng  96  9月  6 00:46 test.c\n```\n可以看到，在源文件`add.c` 、`add.h` 、`test.c` 三个文件大小一致的前提下，最终打包出来的`test` 可执行文件，链接静态库和链接动态库的文件体积相差了近`44`倍！\n\n\n# 总结\n通过以上对比，对链接两种不同的库文件的区别也有了一定的了解，其实并不存在优劣的区别；根据场景，我们可以选择不同的方案，比如使用动态链接库时，需要运行环境中存在对应的库文件，如果使用静态库就不存在这个问题，更方便调用者使用。\n\n\n\n","tags":["Linux","gcc"]},{"title":"Python源代码保护之so","url":"/2019/09/01/python-so/","content":"\nPython的解释特性是将py编译为独有的二进制编码pyc文件，然后对pyc中的指令进行解释执行，但是pyc的反编译却非常简单，可直接反编译为源码，当需要将产品发布到外部环境的时候，源码的保护尤为重要。\n\n基于以上原因，本文将介绍如何将python源码编译pyc，编译成动态链接库.so文件并使用。环境为 [Linux manjaro](https://manjaro.org/)，python版本为3.7，gcc版本为9.1.0。\n\n# py文件打包为so\n\n首先在test目录下新建add.py文件,写入测试代码:\n\n```python\ndef add(a,b):\n    result = a+b\n    print(f'{a}+{b}={result}')\n    return result\n```\n\n新建setup.py文件:\n```python\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(ext_modules=cythonize([\"add.py\"]))\n```\n\n然后执行以下命令\n\n```bash\npython setup.py build_ext\n```\n新的目录结构为:\n\n```bash\n➜  test tree\n.\n├── add.c\n├── add.py\n├── build\n│   ├── lib.linux-x86_64-3.7\n│   │   └── add.cpython-37m-x86_64-linux-gnu.so\n│   └── temp.linux-x86_64-3.7\n│       └── add.o\n└── setup.py\n```\n新生成的build/lib*/add.*.so目录下的就是我们想要的，直接开始测试：\n\n```bash\n➜  test cd build/lib.linux-x86_64-3.7 \n➜  lib.linux-x86_64-3.7 python\nPython 3.7.3 (default, Jun 24 2019, 04:54:02) \n[GCC 9.1.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import add\n>>> c = add.add(1,2)\n1+2=3\n>>> c\n3\n```\n\n由python代码编译成的so，是可以直接当成模块import使用的，如果是其他语言呢？在test根目录下，除了新生成的build文件夹之外，在根目录生还生成了add.c文件，其实编译so的过程，就是cpython帮我们把python代码翻译成c语言的代码,再由gcc编译成so动态链接库。\n\n下一篇将讲解如何用gcc将.c文件编译成动态链接库。\n\n\n\n\n\n","tags":["python","so"]},{"title":"MongoDB 安全防护","url":"/2019/08/21/mongo_safe/","content":"\nMongoDB 是一个高性能，开源，无模式的文档型数据库，是当前noSql数据库产品中最热门的一种。然而MongoDB部署之后的安全性却容易被忽视，其默认配置会让你的数据库处于裸奔状态，没有任何认证，直接暴露在公网里。MongoDB “[赎金事件](https://www.4hou.com/info/news/7581.html)”便由此引起。\n\n针对上述情况，国家网络与信息安全信息通报中心建议采取以下防范措施：\n- 一是修改数据库默认端口或将数据库部署在内网环境中，将MongoDB数据库默认端口（TCP 27017）修改为其他端口；\n- 二是开启MongoDB数据库访问授权；\n- 三是使用SSL加密功能；\n- 四是使用“--blind_ip”选项，限制监听接口IP；\n- 五是开启数据库日志审计功能，记录所有数据库操作；\n- 六是及时做好重要数据备份工作。\n\n## 修改部署环境\n```\nvim /etc/mongo.conf\n\n# network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0\n```\n\n默认端口是27017 ，修改成其他空闲端口即可。需要注意此处的bingIP是指这个mongo的服务端绑定的IP，并不是网上流传的用来限制哪些client的IP去访问。bindIP默认是0.0.0.0本机地址，任意主机都可访问MongoDB 。\n#### 本机部署\nbindIp 设置为127.0.0.1，只有本机连接数据库。\n#### 内网部署\n假如你服务器是阿里云的服务器，一般会有内网ip和外网ip，通过绑定内网IP+阿里云自带的ECS安全组，可以直接将MongoDB与外部网络阻断。\n#### 外网部署\n尽量避免静数据库暴露在公网，如果业务场景需要，建议部署一套VPN，远程只能通过VPN网络隧道进行连接，可以最大程度避免攻击。\n\n## 访问校验和访问控制\n\n```\nvim /etc/mongo.conf\n\nsecurity:\n    authorization: disabled\n```\n访问校验功能需要所有的客户端在连接MongoDB数据库的时候提供凭据，MongoDB提供db.auth()方法进行验证，在使用mongo shell命令的时候，可以在控制台输入验证信息。\n访问控制可以给指定用户创建不同db的多种权限组合，不同用户访问不同的db，并赋予各自的读写权限。解决步骤如下：\n- 关闭权限验证、启动MongoDB\n- 创建MongoDB超级管理员，给予最高权限\n- 分别创建用户，给予各自db的权限\n- 开启权限验证、启动MongoDB\n- 测试\n#### 启动MongoDB\nMongoDB默认是关闭访问校验功能的，只需要启动MongoDB服务就行\n#### 创建管理员\n创建管理员的相关操作都必须先切换到admin库，然后执行createUser命令：\n```\nmongo\n\n> use admin\nswitched to db admin\n> db.createUser({user:\"root\",pwd:\"123rootpwd\",roles:[{role:\"userAdminAnyDatabase\",db:\"admin\"}]})\nSuccessfully added user: {\n\t\"user\" : \"root\",\n\t\"roles\" : [\n\t\t{\n\t\t\t\"role\" : \"userAdminAnyDatabase\",\n\t\t\t\"db\" : \"admin\"\n\t\t}\n\t]\n}\n```\n\n键入以下命令，可以查看创建结果：\n````\nshow users\n````\n\n#### 创建用户\n````\n> use db1\nswitched to db db1\n> db.createUser({user:\"testUser\",pwd:\"123456\",roles:[{role:\"readWrite\",db:\"db1\"}]})\nSuccessfully added user: {\n\t\"user\" : \"testUser\",\n\t\"roles\" : [\n\t\t{\n\t\t\t\"role\" : \"readWrite\",\n\t\t\t\"db\" : \"db1\"\n\t\t}\n\t]\n}\n````\n上面的命令创建了用户名为testUser的用户，拥有db1的读写权限，更多的用户管理操作可以参考[官方文档](https://docs.mongodb.com/manual/reference/method/js-user-management/)，方便随时修改用户权限。\n#### 开启认证，重启MongoDB\n将配置文件中的authorization修改为enable，重启数据库服务。后续所有的数据库操作都需要校验，否则数据库会报错，需要调用db.auth()认证通过后，才能执行MongoDB shell 命令。\n\n````\n➜  blog # mongo \nMongoDB shell version v4.0.4\nconnecting to: mongodb://127.0.0.1:27017\nImplicit session: session { \"id\" : UUID(\"3a76953f-de87-4845-a848-e7ff4b7bb9bb\") }\nMongoDB server version: 4.0.4\n> use db1\nswitched to db db1\n> show dbs;\n2019-08-16T22:05:50.034+0800 E QUERY    [js] Error: listDatabases failed:{\n\t\"ok\" : 0,\n\t\"errmsg\" : \"command listDatabases requires authentication\",\n\t\"code\" : 13,\n\t\"codeName\" : \"Unauthorized\"\n} :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nMongo.prototype.getDBs@src/mongo/shell/mongo.js:67:1\nshellHelper.show@src/mongo/shell/utils.js:876:19\nshellHelper@src/mongo/shell/utils.js:766:15\n@(shellhelp2):1:1\n> \n> db.auth('testUser','wrongpasswd')\nError: Authentication failed.\n0\n> db.auth('testUser','123456')\n1\n> show dbs;\ndb1  0.000GB\n````\n\n## SSL加密\n\n还没有配置，后面在更。\n## 日志记录\n配置文件详解：\n```\nsystemLog:\n   verbosity: 1  #日志等级，0-5，默认0\n   # quiet: false  #限制日志输出，\n   traceAllExceptions: true  #详细错误日志\n   # syslogFacility: user #记录到操作系统的日志级别，指定的值必须是操作系统支持的，并且要以--syslog启动\n   path: /var/log/mongodb/mongod.log  #日志路径。\n   logAppend: true\n   logRotate: rename #rename/reopen。rename，重命名旧日志文件，创建新文件记录；reopen，重新打开旧日志记录，需logAppend为true\n   destination: file\n   timeStampFormat: iso8601-local\n   # component: #各组件的日志级别\n   #    accessControl:\n   #       verbosity: <int>\n   #    command:\n   #       verbosity: <int>\n```\n将日志输出到文件后，可以通过查看文件跟踪数据库启动及运行错误，以便MongoDB出现异常时，及时定位问题并修复。\n## 数据备份\n#### 备份(mongodump)\nMongodb中我们使用mongodump命令来备份MongoDB数据。该命令可以导出所有数据到指定目录中：\n```\nmongodump -h 127.0.0.1:27017 -d db1 -c books -o /var/db/dump/2019-08-17/db1/books/\n```\n一般在备份服务器上，会用脚本文件定时dump数据，需要考虑到多db(-d参数)、多集合(-c参数)的备份。为了避免数据过多占用磁盘空间，还需要能够及时删除旧的备份。\n#### 恢复(mongorestore)\n```\nmongorestore -h 127.0.0.1:27017 -d db1 -c books /var/db/dump/2019-08-17/db1/books/\n```\n最后一个参数就是要恢复的数据文件夹或文件，当需要还原时，可以根据情况对某些数据进行还原。希望我永远用不到这个命令。\n\n\n\n\n\n\n","tags":["mongo"]},{"title":"Hello World","url":"/2019/08/20/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n","tags":["mongo"]}]