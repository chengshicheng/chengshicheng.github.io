[{"title":"基于Redis的分布式限流器","url":"/2020/07/05/redis-cell/","content":"\n# 基于Redis的分布式限流器\n\n## 前言\n\n在高并发系统知识体系中，我们经常会看到这么一句话\n\n> **保护高并发系统的三大利器：缓存、降级和限流**\n\n 那么什么是限流呢，字面意思就是限制流量，避免后台服务遭受超出服务器能承受的请求量。后台系统在部署时，一般都会经过压力测试，能大概知道服务器的处理能力处在一种什么样的水平，如果超过了上线，放任请求进来的话，可能会发生不可控的后果，而通过**限流**，在请求书达到设定的阈值时，多余的请求就可以直接拒绝服务，而不是一直排队请求直到压垮服务，能够在高并发时能够正常处理阈值的请求，而不是拖垮整个服务。\n\n限流的方式有很多，分享下几种常见的Redis解决方案吧。\n\n## 固定窗口法\n\n固定窗口法也可称之为计数法，是限流算法里最简单也是最容易实现的一种算法，比如我们规定，接口A在1分钟内的访问次数不能超过1000个，我们可以统计每分钟内总的请求数量来进行判断，如下图所示：\n\n![Uplj78.png](https://s1.ax1x.com/2020/07/05/UpBNee.md.png)\n\n通过redis那么我们可以这么做：在redis中设置一个A_key，过期时间为60s，在每次有接口A过来时，向redis获取A_key的值，如果A_key不存在或者A_key存在方式且value < 1000，说明请求数在可控范围内，将value值+1后执行后续操作；如果value超过1000时，说明已经超过阈值，此时接口直接返回错误即可。\n\n对应的Python实现代码如下：\n\n```python\nasync def can_pass_by_url(url_key, time_zone=60, max=1000):\n    \"\"\"\n    :param url_key: 用户访问的接口标识\n    :param time_zone: 接口限制的时间段(s)\n    :param max: 限制的时间段内允许多少请求通过\n    \"\"\"\n    redis_conn = await aioredis.create_redis_pool(address='redis://localhost:6379/0', encoding='utf-8')\n    count = await redis_conn.get(url_key)\n    if not count:\n        count = 1\n        await redis_conn.setex(url_key, time_zone, count)\n    if int(count) < max:\n        await redis_conn.incr(url_key)\n        return True\n    return False\n```\n\n这个方法虽然简单，但是无法有效处理临界值的问题，他只关心在这1分钟内的总请求数量，而忽略了临界值附近的突发流量。如果在0:59的时候进来500个合法请求，又在1:01时进来600个请求，在这种情况下，在这前后3s内的请数就已经超过了阈值，但是并没有被限流，服务还是存在被压垮的可能，存在这个问题是由于计数器固定窗口算法的粒度很粗，不够细，所以无法保证任一时间窗口内的请求次数都是小于阈值的。\n\n## 滑动窗口法\n\n为了解决统计精度太低的问题，引入了滑动窗口算法。下面这张图，很好地解释了固定窗口和滑动窗口算法的区别。\n\n![](https://s1.ax1x.com/2020/07/05/Up1iXq.md.png)\n\n一个灰色大方框代表一个统计周期，就是60s，然后我们将时间窗口进行划分，假设分成6格，每格窗口的时间大小是10s，并且每个小窗口都有各自独立的计数器。没经过一个请求，对应的小窗口的计数器+1，没经过10s，时间窗口就向前移动一个格子，只要窗口内包含的所有子窗口总和超过阈值，就拒绝处理新的请求。\n\n由此可见，当滑动窗口的格子划分的越多，那么滑动窗口的滚动就越平滑，限流的统计就会越精确。针对固定窗口法精细度不够的问题，只要我们把窗口划分的足够小（比如1s），就可以顺利触发限流机制。\n\n根据上面滑动窗口的定义，在redis实现限流很简单了：\n\n1. 需要创建60个key，key为秒级的时间戳，value为在当前秒级内的请求计数，过期时间设置为60s，\n2. 统计窗口内的请求总数，即60个key的有效值总和，判断阈值\n\n```python\nasync def can_pass_by_url(url_key, time_zone=60, max=1000):\n    \"\"\"\n    :param url_key: 用户访问的接口标识(即用户在客户端进行的动作)\n    :param time_zone: 接口限制的时间段(s)\n    :param max: 限制的时间段内允许多少请求通过\n    \"\"\"\n    def _genrate_key(url_key, time):\n        return f'{url_key}_{time}'\n\n    redis_conn = await aioredis.create_redis_pool(address='redis://localhost:6379/0', encoding='utf-8')\n\n    now_ts = int(time.time())\n    keys = [_genrate_key(url_key, now_ts - x) for x in range(time_zone)]\n    values = await redis_conn.mget(*keys)\n    count = sum([int(v) for v in values if v is not None])\n    if count < max:\n        key = _genrate_key(url_key, now_ts)\n        if values[0] is None:\n            await redis_conn.setex(key, time_zone, 1)\n        else:\n            await redis_conn.incr(key)\n        return True\n    return False\n```\n\n虽然滑动窗口法避免了时间界限的问题，但是依然无法很好解决细时间粒度上面请求过于集中的问题，就例如限制了1分钟请求不能超过1000次，请求都集中在59s时发送过来，这样滑动窗口的效果就大打折扣。 为了使流量更加平滑，我们可以使用更加高级的漏桶算法和令牌桶算法。\n\n## 漏桶算法\n\n什么是漏桶算法？\n\n漏桶算法的思路不复杂，它先定义一个固定容量的桶，可以将请求想象成是水流，水流可以任意速率流入漏桶中，同时漏桶以固定的速率将水流出。如果流入速度太大会导致水满溢出，溢出的请求被丢弃。\n\n![](https://s1.ax1x.com/2020/07/08/UZjQzV.jpg)\n\n实现的话可以用Redis的固定容量的`list`来实现一个queue队列，queue表示的就是速率限制大小，这里就是当有消息要发送的时候看 queue 中是否还有位置，如果有，那么就将消息放进 queue 中，这个 queue 以 FIFO 的形式提供服务；如果 queue 没有位置了，消息将被抛弃。\n\n在消息被放进 queue 之后，还需要维护一个定时器，这个定时器的周期就是我们设置的频率周期，例如我们设置得频率是 5条/s，那么定时器的周期就是 200ms，定时器每 200ms 去 queue 里获取一次消息，如果有消息，那么就发送出去，如果没有就轮空。\n\n这种实现方式比较复杂，限于篇幅这里就没有实现了。漏桶算法的缺陷也比较明显，即便桶是空的，如果短时间内有大量的突发请求（大水流入桶）时，即便此时服务器没有任何负载，但是流出速率不会变，每个请求也都得在队列中等待一段时间才能被响应。\n\n##    令牌桶算法\n\n令牌桶算法是漏桶算法的改进版，可以支持突发流量。不过与漏桶算法不同的是，令牌桶算法的漏桶中存放的是令牌而不是流量。\n\n那么令牌桶算法是怎么突发流量的呢？\n\n最开始，令牌桶是空的，我们以恒定速率往令牌桶里加入令牌，令牌桶被装满时，多余的令牌会被丢弃。当请求到来时，会先尝试从令牌桶获取令牌（相当于从令牌桶移除一个令牌），获取成功则请求被放行，获取失败则说明达到限流标准，可以阻塞或拒绝请求。\n\n![](https://s1.ax1x.com/2020/07/08/UZX3Ed.jpg)\n\n令牌桶算法既能够将所有的请求平均分布到时间区间内，又能接受服务器能够承受范围内的突发请求，因此是目前使用较为广泛的一种限流算法。\n\n实现方式的话和漏桶算法类似：\n\n1. 创建一个生产者定时器，按照设定的频率定期生成一个令牌放桶里，如果桶满了则丢弃令牌。\n2. 收到一个请求就从桶里取出一个令牌，若桶里没有令牌则拒绝请求\n\n漏桶算法和令牌桶算法最明显的区别是令牌桶算法允许流量一定程度的突发。因为默认的令牌桶算法，取走token是不需要耗费时间的，也就是说，假设桶内有100个token时，那么可以瞬间允许100个请求通过。令牌桶算法由于实现简单，且允许某些流量的突发，对用户友好，所以被业界采用地较多。当然我们需要具体情况具体分析，只有最合适的算法，没有最优的算法。\n\n以上四种redis限流算法的实现，需要反复调用 Redis 查询与计算，一次限流判断需要多次请求较为耗时，而且在高并发的时候，指令执行顺序可能会对限流效果造成一定的误差。因此我们采用编写 Lua 脚本运行的方式，将运算过程放在 Redis 端，使得对 Redis 进行一次原子操作请求就能完成限流的判断。\n\n##  Redis-cell\n\nredis 4.0 以后开始支持扩展模块，[redis-cell](https://github.com/brandur/redis-cell) 是一个用Rust语言编写的基于令牌桶算法的的限流模块，提供原子性的限流功能，并允许突发流量，可以很方便的应用于分布式环境中\n\n### 安装\n\n官方提供了安装包和源码编译两种方式，源码编译要安装Rust环境，比较复杂，这里介绍安装包方式安装：\n\n1. 根据操作系统[下载](https://github.com/brandur/redis-cell/releases)安装包;\n2. 将文件解压到redis能访问到的路径下` /etc/redis/libredis_cell.so`；\n3. 进入 redis-cli，执行命令`module load  /etc/redis/libredis_cell.so`;\n\n###  参数说明\n\n以下命令表示从一个初始值为100的令牌桶中取3个令牌，该令牌桶的速率限制为400次/60秒。\n\n```shell\nCL.THROTTLE test 100 400 60 3\n              \t\t\t\t  ▲      ▲   ▲  ▲  ▲\n             \t \t\t\t   |  \t\t |   \t | \t\t|   └── 3       表示本次要申请的令牌数，不写则默认为 1\n              \t\t\t\t   |   \t\t |     └─┴─── 400  表示在60s内内允许访问的次数是400\n              \t\t\t\t   |   \t   └─────── 100  官方叫max_burst，其值为令牌桶的容量-1， 首次执行时令牌桶会默认填满\n              \t\t\t     └─────────  test   表示redis的key\n```\n\n再来看一下返回参数\n\n```\n127.0.0.1:6379> CL.THROTTLE test 100 400 60 3\n1) (integer) 0      \t#  0 表示允许，1表示拒绝\n2) (integer) 101    #  桶总容量 就是 100+1得到的。\n3) (integer) 98     #  桶剩余空间，取得了3个所以剩下98\n4) (integer) -1     #  如果拒绝了，需要多长时间后再试(桶有空间了，单位秒)\n5) (integer) 0      #  表示多久后令牌桶中的令牌会存满(单位秒)\n```\n\n如果将最后一个参数调大，并且重复执行几次，就可以看到相应参数的变化\n\n```\n127.0.0.1:6379> CL.THROTTLE test 100 400 60 80\n1) (integer) 0\n2) (integer) 101\n3) (integer) 21\n4) (integer) -1\n5) (integer) 12\n127.0.0.1:6379> CL.THROTTLE test 100 400 60 80\n1) (integer) 1\n2) (integer) 101\n3) (integer) 27\n4) (integer) 7\n5) (integer) 10\n127.0.0.1:6379> \n```\n\n可以看到第二次请求80个令牌时，由于桶中令牌数量不足，申请令牌操作直接被拒绝了。\n\n在某些业务场场景下如果需要重试，直接取出返回结果的第四个值`10s`进行sleep即可，如果不想阻塞线程，也可以异步定时任务来重试。\n\n## 后记\n\n本文由浅入深的介绍了常用的限流算法实现，可以根据自己的业务场景选择合适算法，如果选用窗口法的话，最好由lua脚本实现原子性操作，才能保证高并发下的精准限流，但是有了Redis-cell模块以后，就可以轻松对付大多数的限流场景了，简直太方便。\n\n\n\n\n\n\n\n\n\n\n\n","tags":["redis","限流"]},{"title":"Nginx location的使用","url":"/2020/06/06/nginx-location/","content":"\n\n\nNginx是一款自由的、开源的、高性能的HTTP服务器和反向代理服务器，此文主要是记录一下在Nginx配置时，location应该如何配置。\n\n## http区块\n\nNginx的HTTP配置主要包括三个区块，结构如下：\n\n```\nhttp { //这个是协议级别\n　　include mime.types;\n　　default_type application/octet-stream;\n　　keepalive_timeout 65;\n　　gzip on;\n　　　　server { //这个是服务器级别\n　　　　　　listen 80;\n　　　　　　server_name localhost;\n　　　　　　　　location / { //这个是请求级别\n　　　　　　　　　　root html;\n　　　　　　　　　　index index.html index.htm;\n　　　　　　　　}\n　　　　　　}\n}\n```\n\n## location区块\n\nlocation是Nginx服务器非常核心的配置，用于匹配指定的uri（请求uri不包含查询字符串，如`http://localhost:8080/test?id=10`，请求uri是/test）。一般在修改Nginx配置时，大部分也是围绕着location这个配置进行修改。\n\n不部分情况下其实是不需要很复杂的配置的，做个动静分离已经能满足绝大多数单体服务的；而在分布式环境下，某个server中通常需要配置很多location，来将请求分发到不同的微服务下，此时你必须非常熟悉location 的配置和匹配规则，否则真是无从下手。\n\n下面来看一下一个简单的location配置：\n\n```\n\nlocation / {\n     root   home/;\n     index  index.html;\n}\n```\n\n### 基本语法\n\n`location  [ = | ~ | ~* | ^~ | @]  uri  {...}`\n\n意思是可以用 `=`或` ~ `或` ~ `*或` ^~ `或`@ `符号为前缀，当然也可以没有前缀（因为 [A] 是表示可选的 A ； A|B 表示 A 和 B 选一个，上面的样例就属于没有符号前缀的例子），紧接着是 uri ，再接着是{…} 指令块，整个意思是对于满足这样条件的 uri 适用指令块 {…} 的指令。\n\n### 匹配模式\n\n所有的模式可以分为两种，匹配顺序也是基于这两种模式来进行处理的：\n\n- 普通字符串（literal strings） ，是以无前缀、` = `、` ^~ `三种模式的 uri；\n-  正则表达式（regular expression），是以` ~ `或` ~* `前缀两种模式的uri；\n\n### 匹配顺序\n\n1. 普通字符` = `精确匹配。如果发现精确匹配，nginx停止搜索其他匹配\n2. 其他普通字符匹配，先匹配所有普通字符串，将最精确(命中长度)的匹配暂时存储；\n3. 如果第2步中有` ^~ `命中的，则跳过第4步，直接到第5步；\n4. 按照配置文件中的声明顺序进行正则表达式匹配，只要匹配到一条正则表达式，则停止匹配，取正则表达式为匹配结果；\n5. 如果所有正则表达式都匹配不上，则取之前普通字符串中存储的结果；\n6. 如果普通字符串和正则表达式都匹配不上，则报404 NOT FOUND。\n\n总结一下就是： 普通命中顺序无所谓，因为是按命中精准度来确定的  ；正则命中区分顺序，因为是从前往后命中的，命中一个后就不会继续匹配下一个正则。\n\n### example\n\n1. `无前缀`表示：必须以指定模式开始：\n\n   ```\n   server {\n   　　listen 80;\n   　　server_name localhost;\n   　　location /abc {\n   　　　　……\n   　　}\n   }\n   那么，如下是对的：\n   http://baidu.com/abc\n   http://baidu.com/abc?p1\n   http://baidu.com/abc/\n   http://baidu.com/abcde\n   ```\n\n2. `^~` 表示：类似于**无前缀**修饰符的行为，区别是，如果此模式匹配，是会停止搜索正则匹配的，但是会继续搜索普通模式。\n\n3. `=`表示：必须与指定的uri精确匹配\n\n   ```\n   server {\n   　　listen 80;\n   　　server_name localhost;\n   　　location = /abc {\n   　　　　……\n   　　}\n   }\n   那么，如下是对的：\n   http://baidu.com/abc\n   http://baidu.com/abc?p1\n   如下是错的：\n   http://baidu.com/abc/\n   http://baidu.com/abcde\n   ```\n\n4. `~ `表示：指定的正则表达式要区分大小写\n\n   ```\n   server {\n   　　listen 80;\n   　　server_name localhost;\n         \t location\t~   ^/abc$ {\n   　　　　……\n   　　}\n   }\n   那么，如下是对的：\n   http://baidu.com/abc\n   http://baidu.com/abc?p1=11&p2=22\n   如下是错的：\n   http://baidu.com/ABC\n   http://baidu.com/abc/\n   http://baidu.com/abcde\n   ```\n\n5. `~*` 表示：指定的正则表达式不区分大小写\n\n   ```\n   server {\n   　　listen 80;\n   　　server_name localhost;\n   \t\tlocation ~* ^/abc$ {\n   　　　　……\n   　　}\n   }\n   那么，如下是对的：\n   http://baidu.com/abc\n   http://baidu..com/ABC\n   http://baidu..com/abc?p1=11&p2=22\n   如下是错的：\n   http://baidu..com/abc/\n   http://baidu..com/abcde\n   ```\n\n6. `@`表示：这些location区段客户端不能访问，只可以由内部产生的请求来访问，如try_files或error_page等，以error_page为例：\n\n   ```\n       server {\n   　　listen 80;\n   　　server_name localhost;\n   　　error_page 404 @fallback\n         \n        \t location /abc {\n        \t    //检测文件4.html和5.html,如果存在正常显示,不存在就去查找@qwe值  \n               try_files\t/4.html\t\t/5.html  \t@fallback;      \n     \t\t }   \n        \t location @fallback  {\n            \tproxy_pass http://www.baidu.com;    --跳转到百度页面\n         }\n     }\n   ```\n\n   当uri匹配上`/abc`时，会按顺序检测文件的存在性，并且返回找到的第一个文件，最后一项就是跳转到百度，这种写法可以替代原本常用的rewrite，貌似可以提高解析效率；\n\n## root&alias区别\n\nNginx指定文件路径有两种方式root和alias，两种指令有不同的使用方法和作用域，root可以配置在http、server、location、if区块中，但是alias只能配置在location区块中。\n\n\nroot与alias主要区别在于Nginx如何解释location后面的uri，这会使两者分别以不同的方式将请求映射到服务器文件上。\n\n- root的处理结果是：root路径＋location路径\n\n  ```\n  location ^~ /appImg/{\n      root /home/nginx;\n  }\n  ```\n\n  这个location相当于访问服务器上的文件路径：  /home/nginx/appImg/abc.jpg \n\n- alias的处理结果是：使用alias路径替换location路径\n\n  ```\n  location ^~ /appImg/{\n      alias /home/nginx/;\n  }\n  ```\n\n  这个location相当于访问服务器上的文件目录：/home/nginx/abc.jpg(即alias不会使用location后面配置的路径)，**而且如果alias 指定的是目录，后面一定要加上  `/`**，否则会找不到文件。\n\n","tags":["Nginx"]},{"title":"Redis 千万不要乱用KEYS命令","url":"/2020/05/23/redis-key/","content":"\n## 前言\n\nRedis应该是项目开发中的必备组件了，平时定位问题也会直接在客户端执行一些操作，这不前几天隐约听到我隔壁的哥们和产品在沟通问题，沟通完毕后说要查一下某个Redis服务中的数据量，隔壁那兄弟胸口一拍，说这个简单，`keys` 命令一扫就出来了。我感觉不太对，问他现在数据量大概有多大级别，得到的回答是大约四十万，具体数目不太确定。还好有我在【手动狗头】，让他使用`scan`命令，谈笑之间又避免了一起线上事故。\n\n为什么这么说呢，因为`keys`命令一下去，对于这个数据量整个Redis服务都会卡很久，等待该命令返回，其他指令拿不到数据，肯定会引起一些列的连锁反应，请求大面积的阻塞引起超时或者请求未拿到缓存，直接达到数据库，进而引起高CPU占用，分分钟宕机。\n\n## keys 后果\n\n对Redis的特性比较了解的同学应该都知道，Redis的某些操作是单线程的，也就是说其他命令必须等到上一条命令执行结束后才能执行，如果说我们现在正在执行 `keys` 命令，那么其它指令必须等到当前的 `keys` 指令执行完了才可以继续，再加上 `keys` 操作是遍历算法，复杂度是 O (n)，像和这个数据量级别，服务肯定会卡顿，其他指令可能会延时或者超时报错，会引起请求大面积的阻塞和超时，或者大批量请求直接达到数据库，进而引起高CPU占用，应用服务器分分钟宕机。\n\n有些同学可能在开发服务器上经常这么干，好像也没啥问题，毕竟数据量不多嘛，我也经常这么干，但是在正式服务器上这么干，估计就离开除不远了哈哈哈哈。\n\n综合以上特性，Redis 为了解决这个问题，官方的建议是：生产环境屏蔽掉 `keys` 命令。并且在 2.8 版本中加入了指令：scan。\n\n## 替代方案 scan \n\n那我们现在来一下这个命令：\n\n```\n# 用于迭代当前数据库中的数据库键\nSCAN cursor [MATCH pattern] [COUNT count]\n```\n\n相比 `keys` ， `scan`主要有如下特点：\n\n- 复杂度虽然也是 O (n)，但是它是通过游标分步进行的，不会阻塞线程；\n- 提供 limit 参数，可以控制每次返回结果的最大条数，limit 只是对增量式迭代命令的一种提示 (hint)，返回的结果可多可少；\n- 同 keys 一样，它也提供模式匹配功能；\n- 服务器不需要为游标保存状态，游标的唯一状态就是 scan 返回给客户端的游标整数；\n- 返回的结果可能会有重复，需要客户端去重复，这点非常重要；\n- 遍历的过程中如果有数据修改，改动后的数据能不能遍历到是不确定的；\n- 单次返回的结果是空的并不意味着遍历结束，而要看返回的游标值是否为零\n\n最主要的区别就是`scan`一次只会取出部分数据，不会阻塞线程，在数据量大的环境下也可以毫无压力，scan` 命令的使用，这里就不多做赘述，可以参考以下链接文档：\n\n[SCAN 命令参考](http://doc.redisfans.com/key/scan.html)\n\n## Python  scan 使用\n\n由于需要多次执行指令，在具体使用时，还不是很方便，Python 的Redis client 提供了`scan`和`scan_iter` 的扫描方法，用于逐个扫描和批次扫描获取，\n\n由于我司连接Redis 都是用的是二次封装的包，不知道为什么没有提供`scan_iter`方法，所以只能自己在应用层实现`scan_iter`类似的功能，用于迭代键值和累加求和。那么可以自己封装一下，下次需要直接拿来用就行啦，\n\n虽然简单，还是大概记录一下，留着备用吧：\n\n```python\nimport redis\n\nr = redis.StrictRedis(host='localhost', port=6379, db=0)\n\n\ndef get_key_count(match):\n    sum = 0\n    cursor = None\n    while cursor != 0:\n        cursor = cursor if cursor else 0\n        cursor, data = r.scan(cursor=cursor, match=match, count=500)\n        print(cursor, data)\n        sum += len(data)\n    return sum\n\n\nif __name__ == '__main__':\n    print(get_key_count(match='*'))\n\n```\n\n\n\n","tags":["redis"]},{"title":"MySQL隐式转换引发的SQL注入","url":"/2020/05/02/inexplicit-conversion/","content":"\n# 前言\n\n平时写代码时会有针对性的给业务代码做优化，MySQL的隐式转换是非常常见的，如果写sql时，对相关的表结构不是特别熟悉，一不小心就会触发隐式转换，最常见的结果当然就是`索引失效`了。\n\n# 索引失效\n\n## 表结构\n\n```\nmysql> desc test.order;\n+----------+-------------+------+-----+---------+----------------+\n| Field    | Type        | Null | Key | Default | Extra          |\n+----------+-------------+------+-----+---------+----------------+\n| id       | int(11)     | NO   | PRI | NULL    | auto_increment |\n| order_no | varchar(10) | NO   | UNI | NULL    |                |\n| mark     | varchar(45) | YES  |     | NULL    |                |\n+----------+-------------+------+-----+---------+----------------+\n3 rows in set (0.00 sec)\n```\n\n## SQL语句\n\n可以走索引的SQL语句：\n\n```\nselect * from test.order where order_no = '89349';\n```\n\n走全表扫描的SQL语句：\n\n```\nselect * from test.order where order_no = 89349;\n```\n\n## 执行计划\n\n可以两条语句的执行计划，由于索引字段多了隐式转换，所以走的是全表扫描，如果是生产环境数据量比较大的那种，多来几次这种查询，估计服务器都能跑崩溃。\n\n```\nmysql> explain select * from test.order where order_no = '89349'\\G;\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: order\n   partitions: NULL\n         type: const\npossible_keys: order_no_UNIQUE\n          key: order_no_UNIQUE\n      key_len: 12\n          ref: const\n         rows: 1\n     filtered: 100.00\n        Extra: NULL\n1 row in set, 1 warning (0.00 sec)\n```\n\n```\nmysql> explain select * from test.order where order_no = 89349\\G;\n*************************** 1. row ***************************\n           id: 1\n  select_type: SIMPLE\n        table: order\n   partitions: NULL\n         type: ALL\npossible_keys: order_no_UNIQUE\n          key: NULL\n      key_len: NULL\n          ref: NULL\n         rows: 2\n     filtered: 50.00\n        Extra: Using where\n1 row in set, 3 warnings (0.00 sec)\n\nmysql> show warnings\\G;\n*************************** 1. row ***************************\n  Level: Warning\n   Code: 1739\nMessage: Cannot use ref access on index 'order_no_UNIQUE' due to type or collation conversion on field 'order_no'\n```\n\n通过` show warnings`也可以看到对应的告警信息，可能是由于数据类型不对导致的，因此平时在查SQL语句时，如果看到了有warnings信息，还是应该多加留意。\n\n# SQL注入\n\n## 问题定位\n\n最近接手一个项目，由于各种诡异的前置条件，发现了一个很奇怪的BUG，定位过程跳过不表，最终定位到MySQL语句的问题引起的，为了方便理解，我通过上面例子的表结构，简单复现了一下环境，数据表中总共2条数据。\n\n```\nmysql> select * from test.order;\n+----+----------+--------------+\n| id | order_no | mark         |\n+----+----------+--------------+\n|  1 | DY34852  | order_mark a |\n|  2 | HE89349  | order_mark b |\n+----+----------+--------------+\n2 rows in set (0.00 sec)\n\nmysql> select * from test.order where order_no = 0;\n+----+----------+--------------+\n| id | order_no | mark         |\n+----+----------+--------------+\n|  1 | DY34852  | order_mark a |\n|  2 | HE89349  | order_mark b |\n+----+----------+--------------+\n2 rows in set, 2 warnings (0.01 sec)\n\nmysql> select * from test.order where order_no = 1;\nEmpty set, 2 warnings (0.00 sec)\n```\n\n在上面的查询中，where条件中`order_no`传入的依旧是`int`类型，即会数据类型触发隐式转换：\n\n- 当`order_no=0` 的时候，**会查到表中的所有数据**\n\n- 当`order_no!=0` 时，查询不到任何数据\n\n也就是说在字符到整形的这个转换过程中，MySQL认为所有的数据都是等于 0 的，最终暴露出去的数据，就是整个数据表了。\n\n想一想如果是登录接口执行了下面的语句，那酸爽：\n\n```\nselect * from user where username='xxx' and password =0\n```\n\n##  原因分析\n\n出现这个问题的源头，是在获取request请求数据时，由于某些原因body中没有获取到orde_no，通过get方法取值没取到，拿到了默认值整形`0`，从而为BUG埋下了伏笔。问题修复倒是很简单，默认值改成字符型的`\"0\"`就行了，但是背后暴露出来的问题可不小：\n\n- 请求参数没有做非法性验证，必填参数没有时应该直接报错\n- 就算某些原因可不填，默认值一定要特别小心\n- MySQL在隐式转换的转换规则和后果不熟悉\n- 系统存在SQL注入风险\n\n这里针对隐式转换的相关问题，google了一下才发现，MySQL引擎在执行语句时，像我这种order_no是以`DY、HE`等字符为开头的情况，转换为整形时，一律转成`0`，即数据库中所有的行都能满足这个条件，这也是SQL注入的风险点。\n\n##  转换原则\n\n其实MySQL中对类型转换是有自己默认的规则的。[官方的隐式转换](https://dev.mysql.com/doc/refman/5.7/en/type-conversion.html?spm=5176.100239.blogcont47339.5.1FTben)说明文档：\n\n- 两个参数至少有一个是 NULL 时，比较的结果也是 NULL，例外是使用 <=> 对两个 NULL 做比较时会返回 1，这两种情况都不需要做类型转换\n- 两个参数都是字符串，会按照字符串来比较，不做类型转换\n- 两个参数都是整数，按照整数来比较，不做类型转换\n- 十六进制的值和非数字做比较时，会被当做二进制串\n- 有一个参数是 TIMESTAMP 或 DATETIME，并且另外一个参数是常量，常量会被转换为 timestamp\n- 有一个参数是 decimal 类型，如果另外一个参数是 decimal 或者整数，会将整数转换为 decimal 后进行比较，如果\n- 另外一个参数是浮点数，则会把 decimal 转换为浮点数进行比较\n- **所有其他情况下，两个参数都会被转换为浮点数再进行比较**\n\n很显然，我们上面分析的例子，就是走的浮点数比较，只是全部转换成了`0.00`，不同字符转换成浮点数的时候，可以先查一下结果：\n\n```\nmysql> select 'DY34852' = 0;\n+------------------+\n| 'DY34852' = 0.00 |\n+------------------+\n|                1 |\n+------------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> select 'DY34852' = 0.00;\n+------------------+\n| 'DY34852' = 0.00 |\n+------------------+\n|                1 |\n+------------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> select '34T' = 34;\n+------------+\n| '34T' = 34 |\n+------------+\n|          1 |\n+------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> select '34T3' = 34;\n+-------------+\n| '34T3' = 34 |\n+-------------+\n|           1 |\n+-------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> select '34T3' = 343;\n+--------------+\n| '34T3' = 343 |\n+--------------+\n|            0 |\n+--------------+\n1 row in set, 1 warning (0.00 sec)\n```\n\n通过以上操作，对字符串类型转换基本可以得出一下结论：\n\n- 开头非数字的，全部转换为0.00，如`'DY34852' = 0` \n- 开头为数字的，截取到第一个不是字符的位置，如`'34T3' = 34` \n- 纯数字的，直接转换，如`'34'=34` \n\n# 总结\n\n通过上面的文档和实际操作，对字符型转换规则已经有了大概的了解，其他类型的转换可以再看看文档，熟悉一下。其实对于MySQL默认的转换规则，大概过一下有点印象就好，毕竟我们是要避免SQL进行隐式转换，而不是要去遵守他的默认转换规则。只是在碰到类似问题时，知道从哪个方向去分析问题。\n\n\n\n","tags":["mysql"]},{"title":"多进程 - multiprocessing","url":"/2020/04/11/multi-process/","content":"\nPython提供了非常好用的多进程包multiprocessing，只需要定义一个函数，Python会完成其他所有事情。借助这个包，可以轻松完成从单进程到**并发执行**的转换。\n\n##  性能测试\n\n上一篇提到了对于CPU密集型程序，多进程理论上可以显著提高性能，那还是继续用上一篇的示例代码，用`Process` 实现多进程来测试一下性能：\n\n```python\nimport time\nfrom multiprocessing import Process\n\n\ndef add(n):\n    sum = 0\n    while sum < n:\n        sum += 1\n    print(f'sum:{sum}')\n\n\nif __name__ == '__main__':\n    n = 500000000\n    #  单线程\n    start = time.time()\n    add(n)\n    print('run time1: %s  s' % str(time.time() - start))\n\n    # 多进程\n    start = time.time()\n    p1 = Process(target=add, args=[n // 2])\n    p2 = Process(target=add, args=[n // 2])\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n    print('run time2: %s s' % str(time.time() - start))\n\n```\n\n运行结果：\n\n```shell\nsum:500000000\nrun time1: 19.68217897415161  s\nsum:250000000\nsum:250000000\nrun time2: 11.469464540481567 s\n```\n\n很明显看到，不同于上篇中的多线程，多进程明显提升了纯计算程序的性能；多进程也常用于web 应用部署，搭配协程的使用，能大幅度提升web应用的并发性能，这个后面再单独介绍。下面着重介绍一下Process类。\n\n## Process  \n\n####  使用方法\n\nProcess 类可用来在多操作系统平台上创建新进程。和使用 Thread 类创建多线程方法类似，使用 Process 类创建多进程也有以下 2 种方式：\n\n1. 直接创建 Process 类的实例对象，由此就可以创建一个新的进程；\n2. 通过继承 Process 类的子类，创建实例对象，也可以创建新的进程。注意，继承 Process 类的子类需重写父类的 run() 方法。\n\n不仅如此，Process 类中也提供了一些常用的属性和方法，如表 1 所示。\n\n| **属性名或方法名** | **功能**                                                     |\n| :----------------- | ------------------------------------------------------------ |\n| run()              | 第 2 种创建进程的方式需要用到，继承类中需要对方法进行重写，该方法中包含的是新进程要执行的代码。 |\n| start()            | 和启动子线程一样，新创建的进程也需要手动启动，该方法的功能就是启动新创建的线程。 |\n| join([timeout])    | 和 thread 类 join() 方法的用法类似，其功能是在多进程执行过程，其他进程必须等到调用 join() 方法的进程执行完毕（或者执行规定的 timeout 时间）后，才能继续执行； |\n| is_alive()         | 判断当前进程是否还活着。                                     |\n| terminate()        | 中断该进程。                                                 |\n| name属性           | 可以为该进程重命名，也可以获得该进程的名称。                 |\n| **daemon**         | 和守护线程类似，通过设置该属性为 True，可将新建进程设置为“守护进程”。 |\n| pid                | 返回进程的 ID 号。大多数操作系统都会为每个进程配备唯一的 ID 号。 |\n\n和使用 thread 类创建子线程的方式非常类似，使用 方式1中的方法来创建Process 类对象，其本质是调用该类的构造方法创建新进程。Process 类的构造方法格式如下：\n\ndef __init__(self,group=None,target=None,name=None,args=(),kwargs={})\n\n其中，各个参数的含义为：\n\n- group：该参数未进行实现，不需要传参；\n- target：为新建进程指定执行任务，也就是指定一个函数；\n- name：为新建进程设置名称；\n- args：为 target 参数指定的参数传递非关键字参数；\n- kwargs：为 target 参数指定的参数传递关键字参数。\n\n####  守护进程\n\ndaemon 为守护进程标志，其实在多线程中也有daemon守护线程的概念，属性同理，设置daemon属性必须在调用start() 方法之前，否则会报RuntimeError 错误。\n\n- daemon=True。运行时如果如果主进程结束，那么子进程也随之结束运行，如果在守护进程中子进程加了join(timeout)（起到阻塞主进程的作用），那么主进程会等子进程都运行完。\n\n  timeout为等待超时时间，可以不指定：\n\n  - 不指定时，在子进程执行结束后，会自动唤醒主线程。\n  - 指定时，如果守护进程执行时间为t1，会在min(t1,timeout)，主动唤醒主线程\n\n- daemon=False。False为默认值，主进程执行完自己的任务以后，就退出了，此时子进程会继续执行自己的任务。join(timeout)参数作用同上。\n\njoin方法的原理就是调用相应线程/进程的wait方法进行等待操作的，例如主线程中调用了A线程/进程的join方法，则相当于在主线程/进程中调用了B线程/进程的wait方法，当B线程/进程执行完（或者到达等待时间），B会自动调用自身的notifyAll方法唤醒主线程/进程继续执行，从而达到同步的目的。\n\n提到守护进程，就不得不提僵尸进程和孤儿进程的概念了\n\n#### 僵尸进程\n\n**僵尸进程：(父进程没结束，子进程提前结束，父进程没有处理子进程的状态)——-有害，应当避免**\n一个进程使用fork创建子进程，如果子进程退出，而父进程没有调用wait或waitpid获取进程的状态信息，那么子进程的进程描述符仍保存在系统中，这种进程称为僵死进程。\n\n#### 孤儿进程\n\n**孤儿进程：(父进程提前退出，子进程还没结束，子进程成为孤儿进程)——–无害**\n一个父进程退出，而它的一个或着多个子进程还在运行，那么这些子进程将称为孤儿进程。孤儿进程将被init进程（进程号1）所收养, 并由init进程对它们完成状态收集工作。\n\n*参考博客：https://blog.csdn.net/Lovegengxin/article/details/80347468*\n\n##  Pool  \n\n#### 使用方法\n\nmultiprocessing 模块提供了 Pool类，专门用来创建一个进程池。\n\n> multiprocessing.Pool( processes )\n\n进程池可以提供指定数量的进程给用户使用，即当有新的请求提交到进程池中时，如果池未满，则会创建一个新的进程用来执行该请求；反之，如果池中的进程数已经达到规定最大值，那么该请求就会等待，只要池中有进程空闲下来，该请求就能得到执行。其中，processes 参数用于指定该进程池中包含的进程数。如果进程是 None，则默认使用 os.cpu_count() 返回的数字。\n\n#### 属性方法\n\n| 方法名                                                       | 功能                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| apply( func[, args[, kwds]] )                                | 将 func 函数提交给进程池处理。其中 args 代表传给 func 的位置参数，kwds 代表传给 func 的关键字参数。该方法会被阻塞直到 func 函数执行完成。 |\n| apply_async( func[, args[, kwds[, callback[, error_callback]]]] ) | 这是 apply() 方法的异步版本，该方法不会被阻塞。其中 callback 指定 func 函数完成后的回调函数，error_callback 指定 func 函数出错后的回调函数。 |\n| map( func, iterable[, chunksize] )                           | 类似于 Python 的 map() 全局函数，只不过此处使用新进程对 iterable 的每一个元素执行 func 函数。 |\n| map_async( func, iterable[, chunksize[, callback[, error_callback]]] ) | 这是 map() 方法的异步版本，该方法不会被阻塞。其中 callback 指定 func 函数完成后的回调函数，error_callback 指定 func 函数出错后的回调函数。 |\n| imap( func, iterable[, chunksize] )                          | 这是 map() 方法的延迟版本。                                  |\n| imap_unordered( func, iterable[, chunksize] )                | 功能类似于 imap() 方法，但该方法不能保证所生成的结果（包含多个元素）与原 iterable 中的元素顺序一致。 |\n| starmap( func, iterable[,chunksize] )                        | 功能类似于 map() 方法，但该方法要求 iterable 的元素也是 iterable 对象，程序会将每一个元素解包之后作为 func 函数的参数。 |\n| close()                                                      | 关闭进程池。在调用该方法之后，该进程池不能再接收新任务，它会把当前进程池中的所有任务执行完成后再关闭自己。 |\n| terminate()                                                  | 立即中止进程池。                                             |\n| join()                                                       | 等待所有进程完成。                                           |\n\n演示demo：\n\n```python\nfrom multiprocessing import Pool\n\ndef add(n):\n    sum = 0\n    while sum < n:\n        sum += 1\n    print(f'sum:{sum}')\n\nif __name__ == '__main__':\n    #创建包含 4 条进程的进程池\n    with Pool(processes=4) as pool:\n        adds = pool.map(add, (200,400,600,800))\n```\n\n## 结语\n\n多线程和多进程相关的文章只写了大致的应用场景，稍微总结了一点 api 的使用，不过这都不是重点，真正的重点是多线程/进程之间互斥锁和共享数据、以及多线程/进程通信的多重通信方式原理，后面有空再慢慢更。。。\n\nblog在断了三个月之后，终于动手写下了这一篇。\n\n国内疫情终于到尾声了，武汉已于4月8日正式解封。。。又回到了充实而又忙碌的日子，再次致敬在疫情中牺牲的英雄们！！！\n","tags":["python"]},{"title":"Python 多线程探究","url":"/2020/01/17/multi-thread/","content":"\n# Python 多线程探究\n\n### GIL锁\n\n在介绍Python的多线程编程之前，需要想明确一个概念，Python中的多线程是一个假的多线程，在Python的原始解析器CPython中存在着全局解释器锁（Global Interpreter Lock，GIL），GIL在任何时候都确保只有一个Python线程执行，这个过程中会产生互斥锁来限制线程对共享资源的访问，直到解释器遇到I/O操作或者操作次数达到一定数目时才会释放GIL。\n\n### 多线程性能测试\n\n那么多线程性能表现如何？性能测试一下：\n\n```python\nimport time, threading\n\n\ndef add(n):\n    sum = 0\n    while sum < n:\n        sum += 1\n    print(f'sum:{sum}')\n\n\nif __name__ == '__main__':\n    n = 500000000\n\n    #  单线程\n    start = time.time()\n    add(n)\n    print('run time1: %s  s' % str(time.time() - start))\n\n    # 多线程\n    start = time.time()\n    t1 = threading.Thread(target=add, args=[n // 2])\n    t2 = threading.Thread(target=add, args=[n // 2])\n    t1.start()\n    t2.start()\n    t1.join()\n    t2.join()\n    print('run time2: %s s' % str(time.time() - start))\n\n```\n\n执行结果\n\n```bash\nsum:500000000\nrun time1: 37.292006969451904 s\nsum:250000000\nsum:250000000\nrun time2: 37.5808539390564 s\n```\n\n结果表明此处的多线程并没有什么性能提升，反而由于GIL的频繁设置和释放，性能可能还不如单线程。解释的通俗点，以上场景就像在食堂打饭，单线程的时候是一个阿姨负责一个窗口，多线程是一个阿姨负责两个窗口，可是多开的一个窗口，并没有提升工作效率。\n\nGIL是理解了，那么问题来了，多线程意义何在？还是上面那个例子，假设每个同学在打饭的时候，需要到窗口点菜，打菜的过程其实很快，但是有些同学这时候纠结症就犯了，看菜单花个半分钟，点菜纠结半分钟，阿姨表示很无奈，先去别的窗口招呼同学去了。这么一看，好像就比一个窗口要快很多了，感觉阿姨一个人招呼四五个窗口不成问题(´･ᆺ･`)\n\n接下来用延时（ 或者网络请求等耗时操作）来模拟同学们的拖延症，在add()函数中加一行代码`time.sleep(1)`:\n\n```python\n    while sum < n:\n         time.sleep(1)\n         sum += 1\n```\n\n为了避免用时过长，把add()方法入参 n 改成20，运行结果如下：\n\n```python\nsum:20\nrun time1: 20.021007776260376 s\nsum:10\nsum:10\nrun time2: 10.010624170303345 s\n```\n\n运行结果令人欣慰，运行效率提升max，那么仅一行代码，多线程带来的性能提升为何如此之大？这里涉及到一对概念：CPU密集型、IO密集型。\n\n\n\n### 什么是CPU密集型、I/O 密集型？\n\n来自 [stackoverflow  What do the terms “CPU bound” and “I/O bound” mean?](https://stackoverflow.com/questions/868568/what-do-the-terms-cpu-bound-and-i-o-bound-mean)\n\n- CPU密集型\n\n  A program is CPU bound if it would go faster if the CPU were faster, i.e. it spends the majority of its time simply using the CPU (doing calculations). A program that computes new digits of π will typically be CPU-bound, it's just crunching numbers.\n\n  如果CPU变得更快时，程序运行得更快，则该程序是CPU密集型 ，程序的大部分时间都用于CPU（进行计算）。计算π的数值程序就是个典型的CPU密集型，它只用来计算数字。\n\n  \n\n- I / O密集型\n\n  A program is I/O bound if it would go faster if the I/O subsystem was faster. Which exact I/O system is meant can vary; I typically associate it with disk, but of course networking or communication in general is common too. A program that looks through a huge file for some data might become I/O bound, since the bottleneck is then the reading of the data from disk (actually, this example is perhaps kind of old-fashioned these days with hundreds of MB/s coming in from SSDs).\n\n  如果I / O子系统更快，则程序运行得更快，则该程序是I / O密集型。确切的I / O系统的含义可能会有所不同。我通常将其与磁盘联系在一起，但是一般来说，在网络通信中也很常见。在大文件中查找数据的程序很可能是I/O 密集型，因为瓶颈是从磁盘读取数据（实际上，这个例子可能已经过时，因为SSD已经达到了数百MB/S的速度了）\n\n  \n\n### 应用场景\n\n大部分任务都可以分为CPU密集型和I / O密集型。\n\nCPU密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。\n\nCPU密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。\n\n第二种任务的类型是I / O密集型，涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成（因为IO的速度远远低于CPU和内存的速度）。对于I / O密集型任务，任务越多，CPU处于等待的时间就越短，CPU的效率相对会提高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。\n\nIO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。对于IO密集型任务，最合适的语言就是开发效率高的语言，脚本语言是首选。\n\n\n\n总之，Python就不应该用来写CPU密集型的程序，毕竟运行效率实在太低。。。\n\n\n\n### 结语\n\n大多数情况下，性能瓶颈往往不是单一的CPU或者I / O引起的，当两者都存在时，任意一方的速度提升，都能影响到程序最终的执行效率。\n\n在Python语言中有没有更好的解决方案呢？当然是有的，那就是并行编程（parallel programmning），下一篇继续介绍。\n\n","tags":["python"]},{"title":"Flask 开发之Model：Flask-SQLAlchemy","url":"/2020/01/08/flask-sqlalchemy/","content":"### 前言\n\n由于日常工作中经常在`Flask`和`Django`切换，不同的ORM框架属性区别还是有点大的，在`Flask`中使用了常见的`SQLAlchemy`ORM框架，而`SQLAlchemy`的官方文档又相当简洁，仅仅提供了基本的增删改查操作实例。语法倒是不复杂，只是长时间不接触的话，一时半会还真想不起来，因此在这里总结记录一下。\n\n本文使用[`Flask-SQLAlchemy`](https://pypi.org/project/Flask-SQLAlchemy/)，和`SQLAlchemy`相比语法上稍有不同，具体不再赘述。\n\n### 基础操作\n\n##### 查询\n```python\n# 根据主键查询\nUser.query.get(2)\nUser.query.get_or_404(2)\nUser.query.filter(User.phone=='1316418xxxx').all()\nUser.query.filter(User.phone=='1316418xxxx').first()\nUser.query.filter(User.phone=='1316418xxxx').one()\nUser.query.filter(User.phone=='1316418xxxx').one_or_none()\n\n# first()返回的是tuple格式\n>>> User.query.with_entities(User.username,User.phone).filter(User.phone=='1316418xxxx').first()\n('chengshicheng', '1316418xxxx')\n\n# scalar()返回唯一个结果的第一个元素\n>>> User.query.with_entities(User.username,User.phone).filter(User.phone=='1316418xxxx').scalar()\n'chengshicheng'\n```\n需要注意的是`one()`和`scalar()`方法会抛出`MultipleResultsFound`和`NoResultFound`的异常，`one_or_none()`方法会抛出`MultipleResultsFound`异常。使用的时候需要注意捕捉相关异常。\n\n##### 修改\n```python\n# 修改model\nu = User.query.get(2)\nu.phone = '1327777xxxx'\ndb.session.commit()\n\n# update\nUser.query.filter(User.phone == '1316418xxxx').update({'phone':'1327777xxxx'})\ndb.session.comm\n```\n\n##### 删除\n```python\n# 删除model\nu = User.query.get(2)\ndb.session.delete(u)\ndb.session.commit()\n\n# 使用delete()\nUser.query.filter(User.phone=='1316418xxxx').delete(synchronize_session=False)\ndb.session.commit()\n```\n批量数据删除时，使用query.delete()可以绕过ORM体系，能够显著提升性能。\n\nsynchronize_session  选择从会话中操作匹配对象的策略：\n\n- False \n\n    不同步session，直接执行删除操作。，如果被删除的 objects 已经在 session 中存在，在 session commit 或者 expire_all 之前，这些被删除的对象都存在 session 中。可能会导致获取被删除 objects 时出错。不同步可能会导致获取被删除 objects 时出错\n    \n- 'fetch'    \n\n    删除之前从 db 中匹配被删除的对象并保存在 session 中，然后再从 session 中删除\n    \n- 'evaluate'   \n\n    默认值。根据当前的 query criteria 扫描 session 中的 objects，如果不能正确执行则抛出错误\n\n### 复杂查询\n\n##### filter过滤\n```python\n# equal\nUser.query.filter(User.phone == '1316418xxxx').first()\n\n# not equal\nUser.query.filter(User.phone != '1316418xxxx').first()\nUser.query.filter(not_(User.phone == '1316418xxxx')).first()\n\n# in/not in\nUser.query.filter(User.id.in_([1,2,3])).all()\nUser.query.filter(User.id.notin_([1,2,3])).all()\n\n# None 判断\nUser.query.filter(User.phone.is_(None)).all()\nUser.query.filter(User.phone.isnot(None)).all()\n\n# like/ilike\nUser.query.filter(User.name.like('%CHENG%')).all()\nUser.query.filter(User.name.ilike('%cheng%')).all() #ilike(不区分大小写)\nUser.query.filter(User.phone.notlike('%CHENG%')).all()\nUser.query.filter(User.phone.notilike('%cheng%')).all()\n\n# 文本匹配（都可以用like实现）\nUser.query.filter(User.phone.startwith('1316418')).all()\nUser.query.filter(User.phone.endwith('xxxx')).all()\nUser.query.filter(User.phone.contains('6418')).all()\n\n# and_/or_/not_ 分别对应 & | ~ 运算符\nUser.query.filter(User.age < 18).filter(User.register_at < '2019-01-01').all()\nUser.query.filter(User.age < 18, User.register_at < '2019-01-01').all()\nUser.query.filter(and_(User.age < 18, User.register_at < '2019-01-01')).all()\nUser.query.filter((User.age < 18) & (User.register_at < '2019-01-01')).all()\n\nUser.query.filter(or_(User.age < 18, User.age > 60)\nUser.query.filter((User.age < 18) | (User.age > 60))\n\nUser.query.filter(not_(User.age < 18)).all()\nUser.query.filter(~(User.age < 18)).all()\n\n#组合查询\nOrder.query.filter(Order.created_at > '2019-01-01', order.shop_id= 1,\n                not_(Order.status='unpaid'),\n                or_(Order.type==3, Order.type==7)).all()\n```\n\n##### limit offset slice\n```python\nUser.query.offset(5).limit(10).all()\nUser.query.slice(0,2).all()\nUser.query[0:2]\nUser.query[0]\n```\n\n##### order_by 排序\n```python\nfrom sqlalchemy.sql import text\n\nUser.query.order_by(User.id.desc()).all()\nUser.query.order_by(text('-id')).all()\n\n# id逆序，参考slice操作\nUser.query[::-1]\n\nUser.query.order_by(User.id.asc()).all()\nUser.query.order_by(text('id')).all()\n```\n\n##### 聚合\n```python\nfunc.count:统计行的数量\nfunc.avg:求平均值\nfunc.max:求最大值\nfunc.min:求最小值\nfunc.sum:求和\n\n# 用户的平均年龄和最大年龄\nfrom sqlalchemy import func\nUser.query.with_entities(func.avg(User.age),func.max(User.age)).first()\n```\n\n##### group_by\n```python\nfrom sqlalchemy import func\n\n#group_by 按年龄分组统计人数\nUser.query.with_entities(User.age,func.count()).group_by(User.age).all()\n\n#having 分组后结果再过滤\nUser.query.with_entities(User.age,func.count()).group_by(User.age).having(User.age>18).all()\n\n# group_concat拼接\nUser.query.with_entities(User.age,func.group_concat(User.id),func.count()).group_by(User.age).all()\n\n```\n##### join 联表查询\n```python\n\n# 默认外键关联\nOrder.query.filter(Order.status='paid',Order.created_at > '2019-01-01')\n    .join(User, user.phone == '1316418xxxx')\n# 指定外键关联\nOrder.query.filter(Order.status='paid',Order.created_at > '2019-01-01')\n    .join(User, and_(User.id == Order.user_id, user.phone == '1316418xxxx'))\n\n```\n##### subquery 子查询\n\n```python\n>>> subq  = User.query.with_entities(User.id).filter(User.phone=='1316418xxxx').subquery()\n>>> Order.query.filter(Order.created_by == subq.c.id).all()\n>>> print(Order.query.filter(Order.created_by == a.c.id))\nSELECT orders.id AS orders_id, ...\nFROM orders, \n(SELECT users.id AS id FROM users WHERE users.phone = %s) AS anon_1 \nWHERE orders.created_by = anon_1.id\n\n\n>>> u_ids = User.query.with_entities(User.id).filter(User.phone=='1316418xxxx')\n>>> print(Order.query.filter(Order.created_by.in_(u_ids.subquery())))\nSELECT orders.id AS orders_id, ...\nFROM orders \nWHERE orders.created_by IN (SELECT users.id \nFROM users \nWHERE users.phone = %s)\n\n```\n\n##### label 别名\n\n```python\nUser.query.with_entities((User.username + '_' + User.phone).label('userinfo')).all()\nUser.query.with_entities(User.id.label('user_id')).order_by(text('-user_id')).all()\n\nuser_id = User.id.label('user_id')\nUser.query.filter(user_id>4).order_by(user_id.desc()).all()\n\n# 按月统计订单数量(过去一年内的)\nq = Order.query.with_entities(func.date_format(Order.created_date,'%Y-%m').label('month'),func.count())\n\t.group_by('month').order_by('month').limit(12)\n\n>>> print(q)\nSELECT date_format(orders.created_date, %s) AS month, count(*) AS count_1 \nFROM orders GROUP BY month ORDER BY month \n LIMIT %s\n```\n\n目前用的比较多的就是这些，应该满足大部分情况下的查询，后续会持续更新。如有特别复杂的情况，也可以直接执行sql语句。\n\n##### union 、unionall 区别\n\n```python\nfollowed = Post.query.join(\n            followers, (followers.c.followed_id == Post.user_id)).filter(\n                followers.c.follower_id == self.id)\n        own = Post.query.filter_by(user_id=self.id)\n        return followed.union(own)\n```\n\nsql缓存\n\n```\nq=User.query.prefix_with('SQL_CACHE')\nprint(q)\nSELECT SQL_CACHE users.id AS users_id, users.phone AS users_phone, users.username AS users_username, users.password_hash AS users_password_hash, users.is_active AS users_is_active, users.joined AS users_joined, users.last_login AS users_last_login, users.role_id AS users_role_id \nFROM users\n```\n\n","tags":["Flask","SQLAlchemy"]},{"title":"Selenium 爬虫总结","url":"/2019/12/15/spider-summary/","content":"## 1 前言\n最近由于项目需要，需要抓取网络上的一下数据，实现之后写个技术总结吧。其实我个人是不建议将爬虫接入到正式的项目中的，一来是有爬虫就有反爬虫，程序能稳定多久完全靠运气，网站的接口数据或页面格式稍有变动，爬虫就挂了，这种还是比较好解决的；一旦反扒机制升级，那就是噩梦的开始。二来其实会对服务器造成压力，如果爬取数据量大的话，一般网站也许就扛不住了，一旦造成服务器瘫痪，约等于网络攻击，最后一点。也是最重要的，爬虫可能是违法行为，这个取决于你爬取的数据类型和具体用途，也取决于对方想不想追究你的责任。总之一句话，且爬且珍惜。。。\n\n开始爬虫之前，有以下几个建议：\n\n1. 能通过接口拿到的，不要通过浏览器拿。爬虫的目的是要得到数据，能通过API直接拿到，不仅速度快，还不用解析界面。\n2. 不要只盯着PC端看，多看看移动端的网页或者API，会有惊喜。\n\n在确定要爬谁之前，先开个Fiddler,Charles等顺手的抓包工具，PC端，APP端，MobileWeb端，甚至小程序端，看一下能不到找带到现成的API。实在不行页面+API混着用也可以。接口需要认证权限的话还需要再寻找如何绕过去。\n\n一般来说，PC网页端的数据是最难爬的，反爬机制也是做的最好的，移动端的相对来说会弱很多。\n## 2 环境搭建\n\n在PhantomJS停止更新之后，selenium已经不建议使用PhantomJS了，建议使用chrome和firefox浏览器。如果是在自己电脑上运行的话，只要有浏览器就可以，如果要在服务器上运行的话，\n由于服务器没有安装桌面环境，所以浏览器只能运行在headless模式下，记录一下在Ubuntu Server安装浏览器的步骤：\n\n```shell\npip install selenium\n```\n在调用selenium启动浏览器的时候，需要指定浏览器的可执行文件路径，可以用webdriver_manager来替我们管理浏览器。\n```shell\npip install webdriver_manager\n```\n#### 2.1 chrome\n```shell\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\nsudo dpkg -i google-chrome*.deb\n```\n如果环境依赖报错，执行以下命令：\n```shell\nsudo dpkg -i  --force-depends google-chrome*.deb  \nsudo apt-get install -f\n```\n\n#### 2.2 firefox\n```shell\nsudo apt-get install firefox\n```\n\n\n## 3 IP代理\n\n#### 3.1 设置代理\n- requests\n    ```python\n  proxies = {'http': f\"http://{host}:{port}\",\n               'https': f\"http://{host}:{port}\", }\n  s = requests.Session()\n  s.proxies.update(proxies)\n  url = 'http://www.baidu.com'\n  resp = s.get(url)\n  ```\n- selenium+chrome\n    ```python\n    from selenium import webdriver\n    \n    options =  webdriver.chrome.options.Options()\n    options.add_argument(f'--proxy-server=http://{host}:{port}')\n    driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n    driver.get('http://www.baidu.com')\n    ```\n\n- selenium+firefox\n    ```python\n  from selenium import webdriver\n  from webdriver_manager.firefox import GeckoDriverManager\n  \n  fp = webdriver.FirefoxProfile()\n  # Direct = 0, Manual = 1, PAC = 2, AUTODETECT = 4, SYSTEM = 5\n  fp.set_preference(\"network.proxy.type\", 1)\n  fp.set_preference(\"network.proxy.http\", host)\n  fp.set_preference(\"network.proxy.http_port\", int(port))\n  fp.set_preference(\"network.proxy.ssl\", host)\n  fp.set_preference(\"network.proxy.ssl_port\", int(port))\n  fp.update_preferences()\n\n  driver = webdriver.Firefox(GeckoDriverManager().install(),\n                           firefox_profile=fp, options=options)\n  driver.get('http://www.baidu.com')\n  ```\n\n\n\n#### 3.2 匿名ip检验\n在设置代理后，可以访问一下页面，查看ip代理是否生效：http://myip.ipip.net/\n```python\n当前 IP：223.71.7.223  来自于：中国 广东 深圳  电信\n```\n\n到这一步还不够，真正的高匿名ip是不会暴露客户端的ip的。\n终极大杀器：http://httpbin.org/get?show_env=1\n\nHeader内容一览无余，不仅仅可以检验IP代理，还可以查看Header内容伪造是否成功\n```python\n{\n  \"args\": {\n    \"show_env\": \"1\"\n  }, \n  \"headers\": {\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\", \n    \"Accept-Language\": \"en,zh;q=0.9,zh-CN;q=0.8\", \n    \"Dccept-Encoding\": \"none\", \n    \"Dnt\": \"1\", \n    \"Host\": \"httpbin.org\", \n    \"Upgrade-Insecure-Requests\": \"1\", \n    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\", \n    \"X-Forwarded-For\": \"223.71.7.223, 223.71.7.223\", \n    \"X-Forwarded-Port\": \"80\", \n    \"X-Forwarded-Proto\": \"https\", \n    \"X-Real-Ip\": \"223.71.7.223\"\n  }, \n  \"origin\": \"223.71.7.223, 223.71.7.223\", \n  \"url\": \"https://httpbin.org/get?show_env=1\"\n}\n```\n\n非高匿名IP代理请求，在Header中是会包暴露客户端IP的(X-Forwarded-For)，在Header中能同时看到客户端和代理的IP，我的代理是直接付费购买的，还没有遇到过这种情况。\n\n## 4 webdriver设置\n\n在PhantomJS停止更新之后，seleniums已经不建议使用PhantomJS了，不过新版chrome和firefox都支持了headless模式，下面的参数是我用到的，更多参数可以参考官方文档：\n\n#### 4.1 headlss模式\n- selenium+chrome\n    ```python\n  options = webdriver.chrome.options.Options()\n  options.add_argument('--headless')\n   driver = webdriver.Chrome(ChromeDriverManager().install(), chrome_options=options)\n\n  ```\n- selenium+firefox\n    ```python\n  options = webdriver.firefox.options.Options()\n  options.add_argument('--headless')\n  driver = webdriver.Firefox(options=options)\n  ```\n  或者\n  ```python\n  options = webdriver.firefox.options.Options()\n  options.headless = True  \n  driver = webdriver.Firefox(options=options)\n  ```\n  \n#### 4.2 不加载图片\n省去图片能够提高页面的加载速度\n- selenium+chrome\n```python\n# 1允许所有图片；2阻止所有图片；3阻止第三方服务器图片\noptions.add_experimental_option(\"prefs\", {\"profile.managed_default_content_settings.images\": 2,})\n\n```\n- selenium+firefox\n```python\noptions.set_preference('permissions.default.image', 2)\n```\n\n#### 4.3 设置user-agent\n我感觉修改这个用处不大，毕竟热门的useragent可能就那么多，就算反爬策略监测到了某个user-agent频繁请求，总不能给封了吧，封了之后其他正常用户还让不让用了。\n安装fake_useragent: `pip install fake_useragent`，或者自己存一点随机取。\n- selenium+chrome\n```python\nfrom fake_useragent import UserAgent\noptions.add_argument(f'user-agent={UserAgent().chrome}')\n```\n- selenium+firefox\n```python\nfp = webdriver.FirefoxProfile()\nfp.set_preference(\"general.useragent.override\", UserAgent().firefox)\ndriver = webdriver.Firefox(firefox_profile=fp)\n```\n\n#### 4.4 避免爬虫监测\n很多资料显示，selenium控制下的浏览器会被监测到，页面通过js代码获取到的`window.navigator.webdriver` 值不一样，正常手动打开的浏览器获取到的是`undefined`，而用selenium开启的浏览器是`true`。在浏览器中按下F12，点击console\n- 手动打开chrome\n```\n> window.navigator.webdriver\n< undefined\n```\n- selenium打开chrome\n```\n> window.navigator.webdriver\n< true\n```\n\n看来这个问题却是是存在的，至于网站有没有做这个监测，可以看一下js代码。很多资料都是用mitmproxy屏蔽js代码来绕过监测，实现起来也比较麻烦，不过对于chrome浏览器来说，设置成开发者模式就可以很轻松的解决，只需要一行代码\n```python\noptions.add_experimental_option('excludeSwitches', ['enable-automation'])\n```\n\n#### 4.5 其他设置\n- 等待时间，超时时间\n```python\ndriver = webdriver.xxxx()\ndriver.implicitly_wait(20)\ndriver.set_page_load_timeout(20)\n```\nimplicitly_wait是页面等待时长，经过实际验证，如果实际页面x秒(x<20)加载完成，则代码执行结束，否则页面继续等待加载至总时长（20秒）,才回执行后面的流程，\nset_page_load_timeout是超时时间，如果不设置，在find_element方法招不到对应元素的时候程序会一直卡住，所以这个参数一定要设置。\n\n\n## 5 元素解析\n推荐chrome插件xpath helper，鼠标往元素上一放就可以得到xpath，再也不用分析html层级了。本地开发调试时可以先关闭headless模式，可以直观看到页面数据\n\n## 6 其他\n以上示例中，每次新建一个webdriver都会新建一个浏览器窗口，所以代码每次处理完毕之后记得关闭浏览器，否则开多了系统内存也扛不住了。\n\n## 7 demo\n```python\nimport json\nimport time\nfrom selenium import webdriver\nfrom selenium.common.exceptions import NoSuchElementException, TimeoutException\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom fake_useragent import UserAgent\n\nhost = 'xxx.xx.xxx.xx'\nport = 'xx'\n\noptions = webdriver.chrome.options.Options()\noptions.add_argument('--headless')\noptions.add_argument('--no-sandbox')\noptions.add_argument('--disable-gpu')\n# 1允许所有图片；2阻止所有图片；3阻止第三方服务器图片\noptions.add_experimental_option(\"prefs\", {\n    \"profile.managed_default_content_settings.images\": 2,\n})\noptions.add_argument(f'user-agent={UserAgent().chrome}')\n# 开发者模式\noptions.add_experimental_option('excludeSwitches', ['enable-automation'])\n\n# 添加代理服务器信息\noptions.add_argument(f'--proxy-server=http://{host}:{port}')\ndriver = webdriver.Chrome(ChromeDriverManager().install(),\n                          chrome_options=options)\ndriver.implicitly_wait(20)\ndriver.set_page_load_timeout(20)  # 一定要设置超时时间,否则element找不到会一直卡死\ndata = {}\ntry:\n    driver.get('http://httpbin.org/get?show_env=1')\n    pre = driver.find_element_by_xpath(\"/html/body/pre\").text\nexcept NoSuchElementException as e1:\n    print(f'error,browser page:{driver.title}')\nexcept TimeoutException as e2:\n    print(f'webdriver time out')\nelse:\n    data = json.loads(pre)\nfinally:\n    #driver.delete_all_cookies()  # 清除浏览器cookie缓存\n    driver.quit()\n\nprint(data)\n```\n\n","tags":["python","爬虫"]},{"title":"Django自定义中间件","url":"/2019/11/16/django-mid/","content":"## 前言\nDjango中间件是一个常规的Python类，可插入Django的请求/响应生命周期。所有Django请求都会执行注册过的中间件中的方法，自定义的中间件需要继承Middleware类，并在项目设置中注册路径MIDDLEWARE_CLASSES。\n\n自定义中间件类应至少定义以下方法之一：\n\n-  在请求期间调用：\n\t- process_request(request)\n\t- process_view(request, view_func, view_args, view_kwargs)\n\t- process_exception（request，exception)\n\n-  在响应期间调用：\n\t- process_exception(request, exception) \n\t- process_template_response(request, response)\n\t- process_response(request, response)\n\n看方法名称就能看出来都是干什么的了，根据自己需要新建自定义类即可。\n\n## 中间件原理\n在整个请求到相应的过程中，会两次调用Middleware类，所以在设置的时候需要注意中间件的顺序，先看一下我当前项目设置的Middleware类：\n```python\nMIDDLEWARE_CLASSES = [\n    'corsheaders.middleware.CorsMiddleware',\n    'django.middleware.security.SecurityMiddleware',\n    'django.middleware.common.CommonMiddleware',\n]\n```\n在请求周期中，会**自上而下**的执行Middleware类，即首先执行CorsMiddleware类，依次执行到CommonMiddleware类，对于每个中间件，它都会执行process_request()和process_view()方法。\n接下来Django将会在view中执行，完成view应该完成的工作（查询数据库，分页，处理信息，逻辑处理等等），然后为客户端返回`Response`响应。\n\n在响应周期中，将**自下而上**执行Middleware类，即首先执行CommonMiddleware，最后执行 CorsMiddleware。同理，对于每个中间件，它将执行process_exception()，process_template_response()和process_response()方法。\n\n以上都执完毕之后，才回返回响应到客户端。下图是从Django官方文档中提取的，图解很形象。\n[![](https://docs.djangoproject.com/en/1.8/_images/middleware.svg)](https://docs.djangoproject.com/en/1.8/_images/middleware.svg)\n\n\n## 创建中间件类\n在我目前项目中，对于一些捕捉到的异常，是有用log记录异常信息的，但是没有捕捉到的exception，在非development环境中，是看不到的，所以打算加一个全局捕捉异常的中间件，用来trace 异常信息，方便快速定位问题。\n\n首先在lib文件夹下新建一个exception. py文件：\n```\nimport logging\nfrom django.utils.deprecation import MiddlewareMixin\nlogger = logging.getLogger(__name__)\n\nclass LogExceptionMiddleware(MiddlewareMixin):\n\n    def process_exception(self, request, exception):\n        import traceback\n        logger.error(traceback.format_exc())\n\n```\n\n接下来去`setting.py` 文件中添加`LogExceptionMiddleware`到Django中间件类集合中即可：\n```python\nMIDDLEWARE_CLASSES = [\n    ...\n    'lib.exeption.LogExceptionMiddleware'\n]\n\n```\n\n启动服务，随便抛个异常，就能在log文件中看到异常信息了。\n\n","tags":["python","Django"]},{"title":"python网络通信：ctypes和struct使用总结","url":"/2019/09/21/ctypes-struct/","content":"\n最近项目组需要对接一个用C语言开发的so库，so库的函数主要功能是封装了一些socket通信的细节，以及数据加密。so的相关文档介绍了使用方式，主要流程如下：\n1. 初始化环境\n2. 创建socket\n3. 注册函数，socket的返回数据会通过这个函数返回\n4. 通过socket发送字节数据\n5. 接受返回数据，并解析处理\n\n第一次看的时候还是有点懵逼的，首先，动态库是C程序，这个函数是Python程序，C程序能够直接调用Python程序吗？其次，虽然这里不管是C还是Python都涉及到一些基本的数据类型，比如int型变量，C中的int和Python中的int是一样的吗？很显然，这两个问题的答案都是否定的，C程序没法直接调用Python函数，C中的整型和Python中的整型虽然都标记为int，但是两者是不一样的。\n\n这就是[`ctypes`](https://docs.python.org/3.6/library/ctypes.html)库存在的原因了，我们都知道Python是用C语言开发的，具体内部实现细节我们不得而知，但是像上面提到的，int,string等基本的数据类型，如何和C对应起来，要知道C中的类型可比Python复杂多了。ctypes库作为C和Python之间的桥梁，让Python程序也可以和C语言交互了。\n\n## ctypes使用\n\n### 数据类型对应\nctypes定义了许多基本的C兼容数据类型：\n\nctypes type     | C type                                  | Python type\n---             | ---                                     | ---\nc_bool          | _Bool                                   | bool (1)\nc_char          | char                                    | 1-character bytes object\nc_wchar\t        | wchar_t\t                              | 1-character string\nc_byte\t        | char\t                                  | int\nc_ubyte\t        | unsigned char\t                          | int\nc_short\t        | short\t                                  | int\nc_ushort\t    | unsigned short\t                      | int\nc_int\t        | int\t                                  | int\nc_uint\t        | unsigned int\t                          | int\nc_long\t        | long\t                                  | int\nc_ulong\t        | unsigned long\t                          | int\nc_longlong\t    | __int64 or long long\t                  | int\nc_ulonglong\t    | unsigned __int64 or unsigned long long  | int\nc_size_t\t    | size_t\t                              | int\nc_ssize_t\t    | ssize_t or Py_ssize_t\t                  | int\nc_float\t        | float\t                                  | float\nc_double\t    | double\t                              | float\nc_longdouble\t| long double\t                          | float\nc_char_p\t    | char * (NUL terminated)                 | bytes object or None\nc_wchar_p\t    | wchar_t * (NUL terminated)              | string or None\nc_void_p\t    | void *\t                              | int or None\n\n\n\n### 加载so  \nPython通过ctypes加载so有两种方式，如果so文件不在系统环境变量路径中，也可以使用相对路径或者绝对路径来加载，`libc.so.6` 库是linux系统中自带的c函数库，常用的c函数都能通过它调用。\n```python\n>>> from ctypes import *\n>>> libc = CDLL(\"libc.so.6\") \n>>> libc\n<CDLL 'libc.so.6', handle 7fb8246af000 at 0x7fb823860940>\n>>> \n>>> libc=cdll.LoadLibrary(\"/usr/lib/libc.so.6\")\n>>> libc\n<CDLL '/usr/lib/libc.so.6', handle 7f6d0ff77000 at 0x7f6d0f129710>\n```\n\n### 函数调用\n```python\n>>> func=libc.printf\n>>> func\n<_FuncPtr object at 0x7f2320a594f8>\n>>> result = func(c_char_p('hello'))\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: bytes or integer address expected instead of str instance\n>>> \n>>> result = func(c_char_p('hello'.encode()))\nhello>>> \n>>> result\n5\n```\nC的printf()函数接收字节数组的指针`char *`，对应ctypes中的`c_char_p`，直接传入字符串会报错，提示传入`bytes`，转化后成功输出。\n\n总而言之，C类型接收什么类型的参数，Python调用时用ctypes内置的方法转换一下就可以了。其实有一些不用转换也可以直接调用，比如int类型：\n\n```python\n>>> libc.abs(-100)\n100\n```\n\n\n### 指定参数类型\n\n在调用C函数时，可以通过前置的指定类型方法，为我们检查数据格式是否正确，比如有这样一个函数：\n```c\nint CSend_message(char *data,int len)\n```\n入参分别是需要发送的参数，和参数长度，返回值是int类型，代表是否发送成功。在Python中可以用`argtypes`和`restype`来指定其入参和返回值的类型，ctypes会为我们检查输入的参数类型是否正确：\n\n```python\n>>> data = 'to_send_data'.encode()\n>>> \n>>> so = CDLL(\"xxx.so\") \n>>> csend = so.CSend_message\n>>> csend.argtypes = [c_char_p, c_int]\n>>> csend.restype = c_int\n>>> result = csend(data, len(data))\n\n```\n\n当然简单的参数我觉得是用不上这些的，在比较复杂的情况下，还是比较试用的，比如这个：\n```c\nint CSign(unsigned char sourcedata[],unsigned long sourcedataLen,\n          unsigned char targetdata[],unsigned long &targetdataLen);\n          \n参数说明：\n\n1. sourcedata 未签名的数据    \n2. sourcedataLen 未签名数据长度    \n3. targetdata 已签名数据 \n4. targetdataLen 已签名数据长度的地址 \n```\n\n将要加密的参数Sourcedata传入后，该方法会将加密后的128字节的数据存入targetdata所在的内存地址中，要想拿到加密结果，需要我们自己从该地址中读取，这里随意初始化一个128字节的字节数组传入即可：\n\n```python\n>>> data = 'to_sign_data'.encode()\n>>> length = len(data)\n>>> source = tuple(z for z in data)\n>>> source_data  = (c_ubyte * length)(*source)\n>>> source_data\n<__main__.c_ubyte_Array_12 object at 0x7f2320ab29d8>\n>>> \n>>> target_data = (c_ubyte * 128)(1)\n>>> target_data\n<__main__.c_ubyte_Array_128 object at 0x7f2320a7eae8>\n>>> \n>>> sign = so.CSign\n>>> sign.argtypes = [c_ubyte * length, c_ulong,\n                     c_ubyte * 128, c_ulong]\n>>> sign.restype = c_int\n>>> sign(source_data, c_ulong(length),\n         target_data, c_ulong(addressof(c_int(128)))\n         )\n```\n\n### 地址取值\n上面的加密程序执行之后，需要从`target_data` 所在的内存地址中取值。ctypes中的取值我用到了两种，\n一种是根据内存地址来取值，一种是根据指针来取值，其实可以总结为一种，因为指针指向的就是一个内存地址。\n\n- 通过`addressof`方法可以获取ctypes对象的内存地址，再通过`string_at()`取出固定大小的数据\n   从内存地址取值：\n    ```python\n    string_at(addressof(target_data), 128)\n    ```\n- 通过`cast`方法将ctypes对象转化为指针对象，再用`string_at()`取出数据\n    ```python\n    data_p = cast(target_data, c_char_p)\n    string_at(data_p, 128)\n    ```\n\n### 回调函数\n\n#### 创建回调函数\n上面提到socket发送数据之后，server端的数据会通过回调函数返回，文档中对回调函数的定义如下，返回两个参数，第一个是指向数据的指针，第二个是数据的长度：\n\n```c\nint (CALLBACK *ReadCallback)(char *data, int len)\n```\n\n根据要求在Python中新建回调函数：\n```python\ndef callback(data_p, len):\n    message = string_at(data_p, len)\n    parse_message(message)\n    return 0\n```\n\n函数也很简单，就是从指针地址中取出数据，并调用数据解析函数，返回值为int。\n\n#### 注册回调函数\n\n在创建socket客户端的时候，输入服务器的ip和端口，同时传入回调函数，C函数定义如下\n```c\nint CCreate_socket(ReadCallback readcallback,char *ip,u_short port);\n```\n\n在ctypes中，通过关键字 `CFUNCTYPE` 来声明一个回调函数的类型，通过关键字的字面意思也能够看出来，C-FUNC-TYPE，那就是C语言的函数（FUNC）类型（TYPE）。\n```python\nCMPFUNC = CFUNCTYPE(c_int, c_char_p, c_int)\n```\n眼尖的同学可能已经发现了，上面不是求只要获得回传参数的指针和长度么，这里声明回调函数类型的时候怎么变成了三个参数？其实吧，这里只是指的数据类型，并不是参数本身，后面两个 `c_char_p`和`c_int` 指的是传递给 `callback` 回调函数的参数类型，第一个 `c_int` 指的是回调函数的返回值类型。只有这样定义，C程序才能认识。\n\npython相关的代码如下：\n```python\nsocket = ukey.CCreate_socket\nsocket.argtypes = [CMPFUNC, c_char_p, c_ushort]\nsocket.restype = c_int\nc = socket(CMPFUNC(callback), ip, port)\n```\n\n\n## struct打包拆包\n\nstruct是python用来组包解包常用的包，在网络通信中，所有的数据都会通过字节流的方式传输，比如常用的 `https` ，只不过这些通信协议帮我们实现了底层的数据拼接和发送的细节，让开发者只需要关注应用层的业务逻辑。在`socket`通信中，一般都是开发者自定义协议规则。\n\n在ctypes的数据类型对应表中，可以看到C中的数据类型很多，Python中的一个`int`的数据，在组包发送的时候，需要对应C中的哪一种数据类型，到底应该填充几个字节呢？在协议定好的基础上，通过struct可以轻松定义。使用比较简单，只需要掌握`字节序` 和 `字符格式` 两点。\n\n\n#### 字节序\n对于多字节数据，由于在内存中的存储方式的不同，可以分为`大端` 和 `小端`模式；默认情况下，C类型以机器的本机格式和字节顺序表示， struct可以指定具体用哪种模式：\n\n\nCharacter  | Byte order                  | Size\n---        | ---                         | ---\n@          | native                      | native\n=          | native                      | standard  \n<          | little-endian               | standard \n>          | big-endian                  | standard\n!          | network (= big-endian)      | standard\n\n\n#### 字符格式\n\n格式字符具有以下含义；C 和 Python值之间的按其指定类型的转换应当是直接定义具体类型，每种类型的标准大小，就是打包后占用的字节个数：\n\n\n格式        | C类型                | Python类型 | 标准大小 | 注释\n---        | ---                  | ---       | ---    | ---\nx          | 填充字节               | 无         | 1     |  (1),(3)\nc          | char       | 长度为 1 的字节串 | 1   |  (3)\nb          | signed char        | 整数 | 1   |  (1)\nB          | unsigned char      | 整数 | 1   |  (3)\n?          | _Bool      | bool | 2   |  (3)\nh          | short      | 整数 | 2   |  (3)\nH          | unsigned short     | 整数 | 4   |  (3)\ni          | int        | 整数 | 4   |  (3)\nI          | unsigned int       | 整数 | 4   |  (3)\nl          | long       | 整数 | 4   |  (2), (3)\nL          | unsigned long      | 整数 | 8   |  (2), (3)\nq          | long long      | 整数 | 8   |  (4)\nQ          | unsigned long long     | 整数 |    |  (4)\nn          | ssize_t        | 整数 |    |  (5)\nN          | size_t     | 整数 | 2   |  (5)\ne          | (7)        | 浮点数 | 4   |  (5)\nf          | float      | 浮点数 | 8   |  \nd          | double     | 浮点数 |    |  \ns          | char[]     | 字节串 |    |  (6)\np          | char[]     | 字节串 |    |  \nP          | void *     | 整数 |    |  \n\n\n#### pack/unpack示例\n`pack`进行打包，`unpack`进行解包，`calcsize` 用于计算字符格式的大小。常用的就是这3个方法。\n\n\n\n小端模式打包解包`short`、 `unsigned int`和`unsigned long`三个数据:\n```python\n>>> import struct\n>>> struct.pack('<hIL',1,2,3)\nb'\\x01\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00'\n>>> struct.unpack('<hIL',b'\\x01\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00')\n(1, 2, 3)\n>>> struct.calcsize('hIL')\nstr(    struct  \n>>> struct.calcsize('<hIL')\n10\n>>> \n```\n\n大小端的区别：\n```python\n>>> struct.pack('<L', 1)\nb'\\x01\\x00\\x00\\x00'\n>>> struct.pack('>L', 1)\nb'\\x00\\x00\\x00\\x01'\n>>> \n```\n\n格式字符的顺序可能对大小产生影响，因为满足对齐要求所需的填充是不同的:\n```python\n>>> struct.pack('ci', b'*', 7654321)\nb'*\\x00\\x00\\x00\\xb1\\xcbt\\x00'\n>>> struct.pack('ic', 7654321, b'*')\nb'\\xb1\\xcbt\\x00*'\n>>> struct.calcsize('ci')\n8\n>>> struct.calcsize('ic')\n5\n```\n\n格式字符的标准大小会对大小产生影响，区别于是数据的标准大小还是原生大小：\n```python\n>>> struct.calcsize('<hIL')\n10\n>>> struct.calcsize('@hIL')\n16\n```\n\n多个连续的可以在前面加数字，`hhh` 等于 `3h`：\n```python\n>>> struct.pack('hhh', 1,2,3)\nb'\\x01\\x00\\x02\\x00\\x03\\x00'\n>>> struct.pack('3h', 1,2,3)\nb'\\x01\\x00\\x02\\x00\\x03\\x00'\n>>> \n```\n\nstruct还有`pack_into` 和`unpack_from` 用于从固定offset来处理数据，用的不多在这里就不在介绍了。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["python"]},{"title":"Linux gcc编译动态、静态链接库","url":"/2019/09/07/gcc-so/","content":"\n\n# 库的分类\n\n### 动态链接库\n动态库的链接是在程序执行的时候被链接的。所以，即使程序编译完，依赖库仍须保留在系统上，以供程序运行时调用。如果依赖库文件找不到了，动态链接库就无法正常运行了。\n\n###  静态链接库\n静态链接库不受依赖库的影响，即使依赖库被删除了，程序依然可以运行成功。\n\n\n两种库文件应该是各有利弊，静态链接库的本质其实是赋值粘贴的过程，编译过程中会将所使用的库文件一起打包嵌入到可执行文件中。相比之下，链接动态库的程序体积会小很多。\n\n接下类将分别编译静态和动态库，最后动手看看具体有什么区别。首先新建`static` 和`dynamic`文件夹，用于存放各自代码。\n\n\n# 编译动态库\n进入 `dynamic` 文件夹，新建 `add.c` 文件：\n\n```c\n#include <stdio.h>\n\nint add(int a,int b)\n{\n\tint c;\n\tc = a+b;\n   \tprintf(\"from c add() : %d\\n\",c);\n   \treturn c;\n}\n```\n运行以下命令编译，会生成 `libadd.so` 文件：\n\n```\ngcc -fPIC -shared add.c -o libadd.so\n```\n实际上上述过程分为编译和链接两步：\n\n-fPIC是编译选项，PIC是 Position Independent Code的缩写，表示要生成位置无关的代码，这是动态库需要的特性； \n\n-shared是链接选项，告诉gcc生成动态库而不是可执行文件。\n\n上述的一行命令等同于下面两个命令，第一行是把 `add.c` 编译成可执行文件 `add.o`, 如果在C程序中加入 `main()` 函数，是可以直接运行 `add.o` 文件的；第二行是通过 `-shared` 参数将 `add.o` 实现动态链接，得到 `libadd.so` 。\n```\ngcc -c -fPIC add.c\ngcc -shared -o libadd.so add.o\n```\n\n接下来我们将新建 `test.c` 来测试刚刚生成的动态链接库\n首先创建 `add.h` 头文件\n\n```c\nvoid add();\n```\n\n写入测试程序到 `test.c` ：\n\n```\n#include <stdio.h>\n#include \"add.h\"\n \nint main(){\n\tprintf(\"call add in test.c\\n\");\n\tadd(2,3);\n}\n```\n\n编译 `test.c` 文件：\n\n```\ngcc test.c -L. -ladd -o test\n```\n-L的选项是指定编译器在搜索动态库时搜索的路径，告诉编译器 `add.so`库的位置。\".\"意思即当前路径，如果不指定，编译器会找不到依赖库而报错\n\n```\n➜  dynamic gcc test.c -ladd -o test\n/usr/bin/ld: 找不到 -ladd\ncollect2: 错误：ld 返回 1\n```\n\n-l选项是因为\nLinux下的库文件在命名时有一个约定，就是应该以 lib 这3个字母开头，由于所有的库文件都遵循了同样的规范，因此在用 -l 选项指定链接的库文件名时可以省去 lib 这3个字母。例如，gcc 在对 `-ladd ` 进行处理时，会自动去`-L` 指定的文件夹下链接名为 `libadd.so` 的文件。\n\n编译完成后，文件夹下有以下文件：\n\n```zsh\n➜  dynamic ls\nadd.c  add.h  libadd.so  test  test.c\n```\n\n我们刚刚在 `test.c`文件中是定义了 `main()` 函数的，此时编译后的 `test` 应该是可以直接运行的。\n\n```\n➜  dynamic ./test\n./test: error while loading shared libraries: libadd.so: cannot open shared object file: No such file or directory\n➜  dynamic ldd test.so\n\tlinux-vdso.so.1 (0x00007ffc859df000)\n\tlibadd.so => not found\n\tlibc.so.6 => /usr/lib/libc.so.6 (0x00007f72e33fb000)\n\t/lib64/ld-linux-x86-64.so.2 => /usr/lib64/ld-linux-x86-64.so.2 (0x00007f72e35fb000)\n➜  dynamic\n```\n\n报错信息提示找不到` libadd.so` 文件，通过 `ldd` 命令查看依赖库时也提示未找到，最简单的方式是将依赖库文件拷贝到系统目录：\n\n```\n➜  dynamic  sudo cp libadd.so /usr/lib\n[sudo] chengshicheng 的密码：\n➜  dynamic \n➜  dynamic ./test\ncall add in test.c\nfrom c add() : 5 \n➜  dynamic \n```\n\n这样，再次执行就成功了，可以看到在`main()` 函数中调用了`libadd.so`动态库中的方法，并计算成功。\n\n\n# 编译静态库\n\n静态库代码和动态库基本一样,只是gcc编译命令参数和动态链接库的有一些差别。先进入 `static` 文件夹，新建 `add.c` 文件：\n\n```c\n#include <stdio.h>\n\nint add(int a,int b)\n{\n\tint c;\n\tc = a+b;\n   \tprintf(\"from c add() : %d\\n\",c);\n   \treturn c;\n}\n```\n\n编译 `add.c` 文件，会生成 `add.o`文件，注意到这里和编译动态链接库时的命令就是少了`-fPIC`参数：\n\n```\ngcc -c add.c\n```\n生成静态库\n\n```\nar -r libadd.a add.o\n```\n\n现在可以开始测试刚刚生成的静态链接库 `libmyadd.a`，首先新建 `add.h`头文件：\n\n\n```\nvoid add();\n```\n然后新建测试文件 `test.c` ：\n\n```\n#include <stdio.h>\n#include \"add.h\"\n \nint main(){\n\tprintf(\"call add in test.c\\n\");\n\tadd(3,5);\n}\n```\n编译运行\n```zsh\n➜  static gcc test.c -ladd -L. -static -o test\n➜  static ls\nadd.c  add.h  add.o  libadd.a  test  test.c\n➜  static ./test\ncall add in test.c\nfrom c add() : 8\n➜  static \n````\n\n要验证依赖库确实是静态库，我们移除静态库   `libadd.a` 文件试试看：\n\n```zsh\n➜  static mv libadd.a libadd.a.back\n➜  static ls\nadd.c  add.h  add.o  libadd.a.back  test  test.c\n➜  static ./test \ncall add in test.c\nfrom c add() : 8\n➜  static \n\n```\n\n以上，静态链接库经过验证之后没有问题。上文中我们提到过链接静态库会比动态的文件体积大，那我们来对比下，直接上数据：\n\n```\n➜  static ll\n总用量 764K\n-rw-r--r-- 1 chengshicheng chengshicheng  114  9月  6 00:59 add.c\n-rw-r--r-- 1 chengshicheng chengshicheng   13  9月  6 01:10 add.h\n-rw-r--r-- 1 chengshicheng chengshicheng 1.6K  9月  6 01:21 add.o\n-rw-r--r-- 1 chengshicheng chengshicheng 1.7K  9月  7 00:14 libadd.a.back\n-rwxr-xr-x 1 chengshicheng chengshicheng 743K  9月  7 00:21 test\n-rw-r--r-- 1 chengshicheng chengshicheng   96  9月  6 01:09 test.c\n➜  static ll ../dynamic \n总用量 48K\n-rw-r--r-- 1 chengshicheng chengshicheng 114  9月  6 00:46 add.c\n-rw-r--r-- 1 chengshicheng chengshicheng  12  9月  5 23:56 add.h\n-rwxr-xr-x 1 chengshicheng chengshicheng 16K  9月  6 00:46 libadd.so\n-rwxr-xr-x 1 chengshicheng chengshicheng 17K  9月  7 00:18 test\n-rw-r--r-- 1 chengshicheng chengshicheng  96  9月  6 00:46 test.c\n```\n可以看到，在源文件`add.c` 、`add.h` 、`test.c` 三个文件大小一致的前提下，最终打包出来的`test` 可执行文件，链接静态库和链接动态库的文件体积相差了近`44`倍！\n\n\n# 总结\n通过以上对比，对链接两种不同的库文件的区别也有了一定的了解，其实并不存在优劣的区别；根据场景，我们可以选择不同的方案，比如使用动态链接库时，需要运行环境中存在对应的库文件，如果使用静态库就不存在这个问题，更方便调用者使用。\n\n\n\n","tags":["Linux","gcc"]},{"title":"Python源代码保护之so","url":"/2019/09/01/python-so/","content":"\nPython的解释特性是将py编译为独有的二进制编码pyc文件，然后对pyc中的指令进行解释执行，但是pyc的反编译却非常简单，可直接反编译为源码，当需要将产品发布到外部环境的时候，源码的保护尤为重要。\n\n基于以上原因，本文将介绍如何将python源码编译pyc，编译成动态链接库.so文件并使用。环境为 [Linux manjaro](https://manjaro.org/)，python版本为3.7，gcc版本为9.1.0。\n\n# py文件打包为so\n\n首先在test目录下新建add.py文件,写入测试代码:\n\n```python\ndef add(a,b):\n    result = a+b\n    print(f'{a}+{b}={result}')\n    return result\n```\n\n新建setup.py文件:\n```python\nfrom distutils.core import setup\nfrom Cython.Build import cythonize\n\nsetup(ext_modules=cythonize([\"add.py\"]))\n```\n\n然后执行以下命令\n\n```bash\npython setup.py build_ext\n```\n新的目录结构为:\n\n```bash\n➜  test tree\n.\n├── add.c\n├── add.py\n├── build\n│   ├── lib.linux-x86_64-3.7\n│   │   └── add.cpython-37m-x86_64-linux-gnu.so\n│   └── temp.linux-x86_64-3.7\n│       └── add.o\n└── setup.py\n```\n新生成的build/lib*/add.*.so目录下的就是我们想要的，直接开始测试：\n\n```bash\n➜  test cd build/lib.linux-x86_64-3.7 \n➜  lib.linux-x86_64-3.7 python\nPython 3.7.3 (default, Jun 24 2019, 04:54:02) \n[GCC 9.1.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import add\n>>> c = add.add(1,2)\n1+2=3\n>>> c\n3\n```\n\n由python代码编译成的so，是可以直接当成模块import使用的，如果是其他语言呢？在test根目录下，除了新生成的build文件夹之外，在根目录生还生成了add.c文件，其实编译so的过程，就是cpython帮我们把python代码翻译成c语言的代码,再由gcc编译成so动态链接库。\n\n下一篇将讲解如何用gcc将.c文件编译成动态链接库。\n\n\n\n\n\n","tags":["python","so"]},{"title":"MongoDB 安全防护","url":"/2019/08/21/mongo_safe/","content":"\nMongoDB 是一个高性能，开源，无模式的文档型数据库，是当前noSql数据库产品中最热门的一种。然而MongoDB部署之后的安全性却容易被忽视，其默认配置会让你的数据库处于裸奔状态，没有任何认证，直接暴露在公网里。MongoDB “[赎金事件](https://www.4hou.com/info/news/7581.html)”便由此引起。\n\n针对上述情况，国家网络与信息安全信息通报中心建议采取以下防范措施：\n- 一是修改数据库默认端口或将数据库部署在内网环境中，将MongoDB数据库默认端口（TCP 27017）修改为其他端口；\n- 二是开启MongoDB数据库访问授权；\n- 三是使用SSL加密功能；\n- 四是使用“--blind_ip”选项，限制监听接口IP；\n- 五是开启数据库日志审计功能，记录所有数据库操作；\n- 六是及时做好重要数据备份工作。\n\n## 修改部署环境\n```\nvim /etc/mongo.conf\n\n# network interfaces\nnet:\n  port: 27017\n  bindIp: 0.0.0.0\n```\n\n默认端口是27017 ，修改成其他空闲端口即可。需要注意此处的bingIP是指这个mongo的服务端绑定的IP，并不是网上流传的用来限制哪些client的IP去访问。bindIP默认是0.0.0.0本机地址，任意主机都可访问MongoDB 。\n#### 本机部署\nbindIp 设置为127.0.0.1，只有本机连接数据库。\n#### 内网部署\n假如你服务器是阿里云的服务器，一般会有内网ip和外网ip，通过绑定内网IP+阿里云自带的ECS安全组，可以直接将MongoDB与外部网络阻断。\n#### 外网部署\n尽量避免静数据库暴露在公网，如果业务场景需要，建议部署一套VPN，远程只能通过VPN网络隧道进行连接，可以最大程度避免攻击。\n\n## 访问校验和访问控制\n\n```\nvim /etc/mongo.conf\n\nsecurity:\n    authorization: disabled\n```\n访问校验功能需要所有的客户端在连接MongoDB数据库的时候提供凭据，MongoDB提供db.auth()方法进行验证，在使用mongo shell命令的时候，可以在控制台输入验证信息。\n访问控制可以给指定用户创建不同db的多种权限组合，不同用户访问不同的db，并赋予各自的读写权限。解决步骤如下：\n- 关闭权限验证、启动MongoDB\n- 创建MongoDB超级管理员，给予最高权限\n- 分别创建用户，给予各自db的权限\n- 开启权限验证、启动MongoDB\n- 测试\n#### 启动MongoDB\nMongoDB默认是关闭访问校验功能的，只需要启动MongoDB服务就行\n#### 创建管理员\n创建管理员的相关操作都必须先切换到admin库，然后执行createUser命令：\n```\nmongo\n\n> use admin\nswitched to db admin\n> db.createUser({user:\"root\",pwd:\"123rootpwd\",roles:[{role:\"userAdminAnyDatabase\",db:\"admin\"}]})\nSuccessfully added user: {\n\t\"user\" : \"root\",\n\t\"roles\" : [\n\t\t{\n\t\t\t\"role\" : \"userAdminAnyDatabase\",\n\t\t\t\"db\" : \"admin\"\n\t\t}\n\t]\n}\n```\n\n键入以下命令，可以查看创建结果：\n````\nshow users\n````\n\n#### 创建用户\n````\n> use db1\nswitched to db db1\n> db.createUser({user:\"testUser\",pwd:\"123456\",roles:[{role:\"readWrite\",db:\"db1\"}]})\nSuccessfully added user: {\n\t\"user\" : \"testUser\",\n\t\"roles\" : [\n\t\t{\n\t\t\t\"role\" : \"readWrite\",\n\t\t\t\"db\" : \"db1\"\n\t\t}\n\t]\n}\n````\n上面的命令创建了用户名为testUser的用户，拥有db1的读写权限，更多的用户管理操作可以参考[官方文档](https://docs.mongodb.com/manual/reference/method/js-user-management/)，方便随时修改用户权限。\n#### 开启认证，重启MongoDB\n将配置文件中的authorization修改为enable，重启数据库服务。后续所有的数据库操作都需要校验，否则数据库会报错，需要调用db.auth()认证通过后，才能执行MongoDB shell 命令。\n\n````\n➜  blog # mongo \nMongoDB shell version v4.0.4\nconnecting to: mongodb://127.0.0.1:27017\nImplicit session: session { \"id\" : UUID(\"3a76953f-de87-4845-a848-e7ff4b7bb9bb\") }\nMongoDB server version: 4.0.4\n> use db1\nswitched to db db1\n> show dbs;\n2019-08-16T22:05:50.034+0800 E QUERY    [js] Error: listDatabases failed:{\n\t\"ok\" : 0,\n\t\"errmsg\" : \"command listDatabases requires authentication\",\n\t\"code\" : 13,\n\t\"codeName\" : \"Unauthorized\"\n} :\n_getErrorWithCode@src/mongo/shell/utils.js:25:13\nMongo.prototype.getDBs@src/mongo/shell/mongo.js:67:1\nshellHelper.show@src/mongo/shell/utils.js:876:19\nshellHelper@src/mongo/shell/utils.js:766:15\n@(shellhelp2):1:1\n> \n> db.auth('testUser','wrongpasswd')\nError: Authentication failed.\n0\n> db.auth('testUser','123456')\n1\n> show dbs;\ndb1  0.000GB\n````\n\n## SSL加密\n\n还没有配置，后面在更。\n## 日志记录\n配置文件详解：\n```\nsystemLog:\n   verbosity: 1  #日志等级，0-5，默认0\n   # quiet: false  #限制日志输出，\n   traceAllExceptions: true  #详细错误日志\n   # syslogFacility: user #记录到操作系统的日志级别，指定的值必须是操作系统支持的，并且要以--syslog启动\n   path: /var/log/mongodb/mongod.log  #日志路径。\n   logAppend: true\n   logRotate: rename #rename/reopen。rename，重命名旧日志文件，创建新文件记录；reopen，重新打开旧日志记录，需logAppend为true\n   destination: file\n   timeStampFormat: iso8601-local\n   # component: #各组件的日志级别\n   #    accessControl:\n   #       verbosity: <int>\n   #    command:\n   #       verbosity: <int>\n```\n将日志输出到文件后，可以通过查看文件跟踪数据库启动及运行错误，以便MongoDB出现异常时，及时定位问题并修复。\n## 数据备份\n#### 备份(mongodump)\nMongodb中我们使用mongodump命令来备份MongoDB数据。该命令可以导出所有数据到指定目录中：\n```\nmongodump -h 127.0.0.1:27017 -d db1 -c books -o /var/db/dump/2019-08-17/db1/books/\n```\n一般在备份服务器上，会用脚本文件定时dump数据，需要考虑到多db(-d参数)、多集合(-c参数)的备份。为了避免数据过多占用磁盘空间，还需要能够及时删除旧的备份。\n#### 恢复(mongorestore)\n```\nmongorestore -h 127.0.0.1:27017 -d db1 -c books /var/db/dump/2019-08-17/db1/books/\n```\n最后一个参数就是要恢复的数据文件夹或文件，当需要还原时，可以根据情况对某些数据进行还原。希望我永远用不到这个命令。\n\n\n\n\n\n\n","tags":["mongo"]},{"title":"Hello World","url":"/2019/08/20/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n\n","tags":["mongo"]}]